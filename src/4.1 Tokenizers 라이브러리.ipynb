{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "647Bcw6sQgpK",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wba2spjoJEjh",
    "outputId": "ea47bc33-1658-42b4-91a5-ec2f4c2825ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.20.0\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (0.6)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.20.0)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (4.66.6)\n",
      "Collecting xxhash (from datasets==2.20.0)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==2.20.0)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (0.26.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2024.8.30)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.20.0)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==2.20.0) (0.2.0)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets==2.20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEa6mcKv5sZU"
   },
   "source": [
    "# 4.1.1 Tokenizers 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393,
     "referenced_widgets": [
      "c7b74c0db4d549978bf3f92d16e13af7",
      "89b1383583d94912b6b39555602d24ce",
      "4a30d1810fc3441faca51d7533e4cb89",
      "cbddcf04773b4a93aa2d8e5e9834c33c",
      "8d0b2dd347b64df191edfd8041fcc9da",
      "22e595e08d3f453f9259feacd29abd8e",
      "0851fec91bae4872a253bbdbde679606",
      "aeb62017ef7f4555af0252dc19d7d560",
      "f7cfebafd60042168b885ebad6216142",
      "0056fd7431ee4976862ba69b5a7253ee",
      "cbb30caff3ef42099e85c6467f1ee183",
      "7284f1135c9f4396967ad5f1f9e2393c",
      "018eb80f40fa4336a58507416f269de1",
      "829b3bad5c924418af2417c412b15314",
      "2f7037ebee38445ba1ef4d5c5ef500ae",
      "d0ea10be9a974646948006a34166396d",
      "c4fd51da9cda47cab4d686e59d4633d3",
      "537ecf412c4c40489ac3a80d871c1b56",
      "9a6048431a25434b8c052b2bb859b362",
      "ff777616b5f2430fab6eb5cca3a762e7",
      "1e58ced59ea540008a09527788e10969",
      "41707d63c8084eb88fad47a513e3f93e",
      "11583281cb3c498ea3a1f6c84e0b1ff0",
      "90be1904369e458c8b65a84209679b01",
      "54d338656f6a4a44bc5c792b54853c7d",
      "5de7eb22f6d84fd584ddc077d89aa92b",
      "05b55f29f7ae4eb3b58e661a20af1f0c",
      "ae34b79070174b589327eedc7ddbba96",
      "184af4b42ce2495fa63c7c50f51ba6d7",
      "af924f6c466e47f8a600dea4b3842c54",
      "204e982c9beb4bcb9cfcf3954662caf2",
      "7d14cc8ada1f445299b1ce1463c538f4",
      "af73dff01e5a40ebb79d1562d036e73e",
      "70e4b5ae20f044e5938501e42431e402",
      "60a51bbba7604ac290883a8f7c27ddeb",
      "0f95b4a5bf2b4461899eb603548383a1",
      "4c223a820ee84697a59bd7c93b24981e",
      "65c78a7a5fb246168160f471e47a368a",
      "f8e8f4bdb5d14574bc37148b32c99d35",
      "1f0416cf835f4540a882b093f9494e2f",
      "b844d084f2ca4009bf80bddef2a44ce3",
      "7e345d1b90954215ace7662089c395ff",
      "5853ce5a74d44e02980a013d4a77a966",
      "b00f47c5b8b1458db6eb2d8e7692e26b",
      "4ddfbb2f64ae4035a3d84551fc7b2418",
      "c72fc8f5dbf3486ebc7b3bde3e77a107",
      "009fed49631a4de594074ac5210a6065",
      "0543e83a24d9403f82669c86e8aa7bca",
      "59187707f1584683b361e7fb16957147",
      "434de8dc72d448259c17af31403ec629",
      "8c620a576a8b4b58a2795b30d052ac15",
      "7c35b275984c4837bd109d93972afa98",
      "8184d477aa8c4941ba101f269b9f3bbb",
      "7db35a29a8434288a0a3d09b83f95700",
      "c357396304354bf381025119fe9f59a3"
     ]
    },
    "id": "aB3qnglMeSOB",
    "outputId": "675bf957-4a88-49ea-e5bb-d8e76e017ac3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b74c0db4d549978bf3f92d16e13af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/22.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7284f1135c9f4396967ad5f1f9e2393c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.17M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11583281cb3c498ea3a1f6c84e0b1ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/847k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e4b5ae20f044e5938501e42431e402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/45678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddfbb2f64ae4035a3d84551fc7b2418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/9107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'guid': 'ynat-v1_train_00000',\n",
       " 'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영',\n",
       " 'label': 3,\n",
       " 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947',\n",
       " 'date': '2016.06.30. 오전 10:36'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"klue\", \"ynat\")\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ptims1Jj7r5b"
   },
   "outputs": [],
   "source": [
    "target_key = \"title\"\n",
    "for key in dataset.column_names.keys():\n",
    "  with open(f\"/content/tokenizer_data_{key}.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(dataset[key][target_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9SAGAmFBxQ_",
    "outputId": "3af7ef3a-5a5d-4a29-a302-bc6e5cbde3af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '[UNUSED0]', '[UNUSED1]', '[UNUSED2]', '[UNUSED3]', '[UNUSED4]']\n"
     ]
    }
   ],
   "source": [
    "user_defined_symbols = [\n",
    "    \"[PAD]\",  # 문장의 길이를 맞추기 위해 사용되는 토큰\n",
    "    \"[UNK]\",  # 토크나이저가 인식할 수 없는 토큰\n",
    "    \"[CLS]\",  # bert 계열 모델에서 문장 전체의 정보를 저장하는 토큰\n",
    "    \"[SEP]\",  # bert 계열 모델에서 문장 구분을 위해 사용하는 토큰\n",
    "    \"[MASK]\", # MLM 모델에서 토큰 마스킹을 위해 사용하는 토큰\n",
    "]\n",
    "\n",
    "unused_token_num = 100\n",
    "unused_list = [f\"[UNUSED{i}]\" for i in range(unused_token_num)]  # 사전학습 시, 어휘에 없는 토큰을 추가하기 위한 빈 공간\n",
    "\n",
    "whole_user_defined_symbols = user_defined_symbols + unused_list\n",
    "print(whole_user_defined_symbols[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nZDjyU5OEhT8"
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "raT7AibRGAwd",
    "outputId": "89eaae9d-9d5f-494a-c54e-2a9f4205339c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'hello howwnare u? '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import normalizers\n",
    "\n",
    "normalizer = normalizers.BertNormalizer()\n",
    "bert_tokenizer.normalizer = normalizer\n",
    "\n",
    "normalizer.normalize_str(\"Héllò hôwWnare ü? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "di517j4CG3VS",
    "outputId": "cc2b26ad-63ab-4512-986d-79a3a480d676"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('안녕하세요', (0, 5)),\n",
       " ('.', (5, 6)),\n",
       " ('제대로', (7, 10)),\n",
       " ('인코딩이', (11, 15)),\n",
       " ('되는지', (16, 19)),\n",
       " ('확인', (20, 22)),\n",
       " ('중입니다', (23, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "pre_tokenizer = Whitespace()\n",
    "bert_tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "pre_tokenizer.pre_tokenize_str(\"안녕하세요. 제대로 인코딩이 되는지 확인 중입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GinaMvDpHSOQ"
   },
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(t, i) for i, t in enumerate(user_defined_symbols)]\n",
    ")\n",
    "\n",
    "bert_tokenizer.post_processor = post_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qMSjdVN5PvJ4"
   },
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "vocab_size = 24000\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    special_tokens=whole_user_defined_symbols,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Whv3bR4pJsso"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "bert_tokenizer.train(glob(f\"/content/*.txt\"), trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "cCNryZnCMElU",
    "outputId": "9cc65e43-3b1c-4c15-a3bd-2855052ed33f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 675, 906, 2220, 4518, 1240, 906, 2220, 569, 6727, 12916, 10780, 586, 1881, 16617, 10191, 106, 3]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'인 ##코 ##딩 및 디 ##코 ##딩 ##이 제대로 이루 ##어지는 ##지 확인 중이 ##ᆸ니다 .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = bert_tokenizer.encode(\"인코딩 및 디코딩이 제대로 이루어지는지 확인 중입니다.\")\n",
    "print(output.ids)\n",
    "\n",
    "bert_tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "VY8ua_-kS09V",
    "outputId": "1d7b97a1-94ab-4853-d564-5251a7d16a76"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'인코딩 및 디코딩이 제대로 이루어지는지 확인 중입니다.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import decoders\n",
    "\n",
    "bert_tokenizer.decoder = decoders.WordPiece()\n",
    "bert_tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctPQEZkrvyxk",
    "outputId": "473ff8b1-495e-4c89-8739-c6e4a7d972d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 675, 906, 2220, 4518, 1240, 906, 2220, 569, 6727, 12916, 10780, 586, 1881, 16617, 10191, 106, 3]\n",
      "[CLS] 인코딩 및 디코딩이 제대로 이루어지는지 확인 중입니다. [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "fast_tokenizer = BertTokenizerFast(tokenizer_object=bert_tokenizer)\n",
    "encoded = fast_tokenizer.encode(\"인코딩 및 디코딩이 제대로 이루어지는지 확인 중입니다.\")\n",
    "decoded = fast_tokenizer.decode(encoded)\n",
    "print(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opzBuKJVxnS4"
   },
   "source": [
    "Save Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "if3ZaaH6wyG_",
    "outputId": "97b579ae-4aee-4cac-8581-7d601f56d16d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('drive/MyDrive/MyTokenizer/tokenizer_config.json',\n",
       " 'drive/MyDrive/MyTokenizer/special_tokens_map.json',\n",
       " 'drive/MyDrive/MyTokenizer/vocab.txt',\n",
       " 'drive/MyDrive/MyTokenizer/added_tokens.json',\n",
       " 'drive/MyDrive/MyTokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"/content/MyTokenizer\"\n",
    "fast_tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s8ItiIklybrB",
    "outputId": "27a2a9ae-adaa-427d-9e32-a31035dc01ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids [[2, 675, 906, 2220, 1675, 6464, 586, 1881, 3], [2, 18632, 1594, 6985, 3782, 3]]\n",
      "token_type_ids [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]\n",
      "attention_mask [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]\n",
      "[CLS] 인코딩 잘 되는지 확인 [SEP]\n",
      "[CLS] 안되면 다시 학습하자 [SEP]\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer = BertTokenizerFast.from_pretrained(output_dir)\n",
    "\n",
    "encoded = new_tokenizer([\"인코딩 잘 되는지 확인\", \"안되면 다시 학습하자\"])\n",
    "\n",
    "for k, v in encoded.items():\n",
    "  print(k, v)\n",
    "\n",
    "print(new_tokenizer.decode(encoded[\"input_ids\"][0]))\n",
    "print(new_tokenizer.decode(encoded[\"input_ids\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mx547DHji20S"
   },
   "source": [
    "# 4.1.2 모델 초기화 후 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "dataset = load_dataset(\"klue\", \"ynat\")\n",
    "model_name = \"/content/MyTokenizer\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "cfg = BertConfig\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycfg = BertConfig(vocab_size=tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "\n",
    "model = BertForMaskedLM(mycfg)\n",
    "print(model.config)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }

 },
 "nbformat": 4,
 "nbformat_minor": 4
}
