{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2ead5796-0f8d-4901-96c3-bf7873f3264c",
      "metadata": {
        "id": "2ead5796-0f8d-4901-96c3-bf7873f3264c"
      },
      "source": [
        "# install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd941830-2673-46a1-8d40-cb25774c0259",
      "metadata": {
        "id": "bd941830-2673-46a1-8d40-cb25774c0259"
      },
      "outputs": [],
      "source": [
        "!pip install \\\n",
        "     scikit-learn==1.4.2 \\\n",
        "     transformers==4.43.0 \\\n",
        "     datasets==2.20.0 \\\n",
        "     peft==0.10.0 \\\n",
        "     accelerate==0.32.1 \\\n",
        "     bitsandbytes==0.43.1 \\\n",
        "     trl==0.9.6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5496dae6-0c2d-46c3-aee5-af396d044b7b",
      "metadata": {
        "id": "5496dae6-0c2d-46c3-aee5-af396d044b7b"
      },
      "source": [
        "# 8.3 Reward Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b99536f-4868-4b5e-9f5c-fd3f8d5a9c2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b99536f-4868-4b5e-9f5c-fd3f8d5a9c2f",
        "outputId": "c1858434-a7ea-4c4c-951b-6c397c922136"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-350m and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = \"facebook/opt-350m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc7c30d-8092-4fdf-9eb1-15a37d344bda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bc7c30d-8092-4fdf-9eb1-15a37d344bda",
        "outputId": "7eca4dcc-c806-4334-a57a-5c142d44b826"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['chosen', 'rejected'],\n",
              "    num_rows: 10000\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:10000]\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ebb1a8-aa5f-4c38-9d47-2fe468d7858b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25ebb1a8-aa5f-4c38-9d47-2fe468d7858b",
        "outputId": "66caac7d-a642-4879-8636-04a2e03d8d8c",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
              "    num_rows: 9701\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preprocess(batch):\n",
        "    result = {\n",
        "        \"input_ids_chosen\": [],\n",
        "        \"attention_mask_chosen\": [],\n",
        "        \"input_ids_rejected\": [],\n",
        "        \"attention_mask_rejected\": [],\n",
        "    }\n",
        "    for chosen, rejected in zip(batch[\"chosen\"], batch[\"rejected\"]):\n",
        "        tokenized_chosen = tokenizer(chosen)\n",
        "        tokenized_rejected = tokenizer(rejected)\n",
        "\n",
        "        result[\"input_ids_chosen\"].append(\n",
        "            tokenized_chosen[\"input_ids\"]\n",
        "        )\n",
        "        result[\"attention_mask_chosen\"].append(\n",
        "            tokenized_chosen[\"attention_mask\"]\n",
        "        )\n",
        "        result[\"input_ids_rejected\"].append(\n",
        "            tokenized_rejected[\"input_ids\"]\n",
        "        )\n",
        "        result[\"attention_mask_rejected\"].append(\n",
        "            tokenized_rejected[\"attention_mask\"]\n",
        "        )\n",
        "\n",
        "    return result\n",
        "\n",
        "dataset = dataset.map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    num_proc=2,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "max_length = 512\n",
        "dataset = dataset.filter(\n",
        "    lambda x: (\n",
        "        len(x[\"input_ids_chosen\"]) <= max_length\n",
        "        and len(x[\"input_ids_rejected\"]) <= max_length\n",
        "    )\n",
        ")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b5cfa2f-30ae-4412-8291-517d8450bf35",
      "metadata": {
        "id": "8b5cfa2f-30ae-4412-8291-517d8450bf35"
      },
      "outputs": [],
      "source": [
        "from trl import RewardTrainer, RewardConfig\n",
        "\n",
        "config = RewardConfig(\n",
        "    logging_dir=\"/content/logs\",\n",
        "    output_dir=\"/content/ckpt\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=5e-5,\n",
        "    optim=\"adamw_torch\",\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = RewardTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=config,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15a35476-651a-40f6-869c-e8151aa98afa",
      "metadata": {
        "id": "15a35476-651a-40f6-869c-e8151aa98afa"
      },
      "source": [
        "# 8.4 SFT: Supervised Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3bc9d7a-530c-4ebf-bb19-3729bb240181",
      "metadata": {
        "id": "f3bc9d7a-530c-4ebf-bb19-3729bb240181"
      },
      "source": [
        "## 기본 구조"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b12a2fd8-d077-477d-8a90-5cf862d5238a",
      "metadata": {
        "id": "b12a2fd8-d077-477d-8a90-5cf862d5238a"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "dataset = load_dataset(\"imdb\", split=\"train\")\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    dataset_text_field=\"text\",\n",
        "    output_dir=\"/content/ckpt\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "trainer = SFTTrainer(\n",
        "    \"facebook/opt-350m\",\n",
        "    train_dataset=dataset,\n",
        "    args=sft_config,\n",
        ")\n",
        "# trainer.train()  # 실제 학습할때만 사용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BWNDXuV9mEH0",
      "metadata": {
        "id": "BWNDXuV9mEH0"
      },
      "source": [
        "## DatacollatorForCompletionOnlyLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d5b3e6-2289-4d05-8f41-079bc4bfc219",
      "metadata": {
        "id": "88d5b3e6-2289-4d05-8f41-079bc4bfc219",
        "outputId": "00b09925-3804-4225-cbfb-de6a043cc2db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100,   42,   16, 1263,    4,   16,   24,  173,  116,    2])\n",
            "only response:  this is response. is it work?</s>\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "collator = DataCollatorForCompletionOnlyLM(\n",
        "    response_template=\" [/INST]\",\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "prompt_example = \"<s>[INST] this is input prompt [/INST] this is response. is it work?</s>\"\n",
        "example = collator([tokenizer(prompt_example)])\n",
        "\n",
        "label = example.labels[0]\n",
        "print(label)\n",
        "print(\"only response:\", tokenizer.decode(label[label > 0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e068b0c-9f29-4106-a86d-8d31af46d918",
      "metadata": {
        "id": "5e068b0c-9f29-4106-a86d-8d31af46d918",
        "outputId": "ea9c5911-d853-4a67-ab42-0d93d48a6877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('[', 10975), ('INST', 39236), (']', 742), ('Ġthis', 42), ('Ġis', 16), ('Ġinput', 8135), ('Ġprompt', 14302), ('Ġ[/', 48651), ('INST', 39236), (']', 742), ('Ġthis', 42), ('Ġis', 16), ('Ġresponse', 1263), ('.', 4), ('Ġis', 16), ('Ġit', 24), ('Ġwork', 173), ('?', 116)]\n",
            "[('[/', 48505), ('INST', 39236), (']', 742)]\n"
          ]
        }
      ],
      "source": [
        "def print_tokens_with_ids(txt):\n",
        "    tokens = tokenizer.tokenize(txt, add_special_tokens=False)\n",
        "    token_ids = tokenizer.encode(txt, add_special_tokens=False)\n",
        "    print(list(zip(tokens, token_ids)))\n",
        "\n",
        "prompt = \"[INST] this is input prompt [/INST] this is response. is it work?\"\n",
        "print_tokens_with_ids(prompt)\n",
        "\n",
        "response_template = \"[/INST]\"\n",
        "print_tokens_with_ids(response_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f81d091-1228-4c3a-b54b-fb72ee7d16ad",
      "metadata": {
        "id": "2f81d091-1228-4c3a-b54b-fb72ee7d16ad",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "setup chat format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9044113-476e-46d5-908c-fafbe45ba17d",
      "metadata": {
        "id": "a9044113-476e-46d5-908c-fafbe45ba17d",
        "outputId": "9e02c07f-4641-43bf-dd0f-f5e3fb46c6a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "before: None\n",
            "after: {% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
            "' + message['content'] + '<|im_end|>' + '\n",
            "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from trl import setup_chat_format\n",
        "\n",
        "model_name = \"facebook/opt-350m\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(\"before:\", tokenizer.chat_template)\n",
        "\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "print(\"after:\", tokenizer.chat_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e318f77a-aae9-4f95-8f5a-1ba9b764f7ae",
      "metadata": {
        "id": "e318f77a-aae9-4f95-8f5a-1ba9b764f7ae",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "formatting func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd120275-41b6-4a03-a11a-999b03a4fcb2",
      "metadata": {
        "id": "bd120275-41b6-4a03-a11a-999b03a4fcb2"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "example = [\n",
        "    {\"question\": \"질문 1\", \"answer\": \"답변 1\"},\n",
        "    {\"question\": \"질문 2\", \"answer\": \"답변 2\"},\n",
        "    {\"question\": \"질문 3\", \"answer\": \"답변 3\"},\n",
        "]\n",
        "test_dataset = Dataset.from_list(example)\n",
        "\n",
        "def formatting_prompts_func(sample):\n",
        "    output_texts = []\n",
        "    for i in range(len(sample[\"question\"])):\n",
        "        text = (\n",
        "            f\"### Question: {sample[\"question\"][i]}\\n \"\n",
        "            f\"### Answer: {sample[\"answer\"][i]}\"\n",
        "        )\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    dataset_text_field=\"text\",\n",
        "    output_dir=\"/content/ckpt\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "trainer = SFTTrainer(\n",
        "    model,\n",
        "    args=sft_config,\n",
        "    train_dataset=dataset,\n",
        "    formatting_func=formatting_prompts_func,\n",
        ")\n",
        "\n",
        "# trainer.train()  # 실제 학습할때만 사용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1423f90d-e10a-4d3d-af75-1b6e233ecf81",
      "metadata": {
        "id": "1423f90d-e10a-4d3d-af75-1b6e233ecf81"
      },
      "source": [
        "## packing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8975e9d9-7f94-4913-8992-29b13271fec7",
      "metadata": {
        "id": "8975e9d9-7f94-4913-8992-29b13271fec7"
      },
      "outputs": [],
      "source": [
        "sft_config = SFTConfig(\n",
        "    packing=True,\n",
        "    max_seq_length=512,\n",
        "    dataset_text_field=\"text\",\n",
        "    output_dir=\"/content/ckpt\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    \"facebook/opt-350m\",\n",
        "    train_dataset=dataset,\n",
        "    args=sft_config\n",
        ")\n",
        "\n",
        "# trainer.train()  # 실제 학습할때만 사용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f4890b-2c56-44aa-bc4a-fbdc934491bb",
      "metadata": {
        "id": "28f4890b-2c56-44aa-bc4a-fbdc934491bb"
      },
      "source": [
        "## model_init_kwargs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5524b214-0dfc-4813-85d8-dd9752918da7",
      "metadata": {
        "id": "5524b214-0dfc-4813-85d8-dd9752918da7"
      },
      "outputs": [],
      "source": [
        "sft_config = SFTConfig(\n",
        "    model_init_kwargs={\n",
        "        \"torch_dtype\": \"auto\",\n",
        "    },\n",
        "    max_seq_length=512,\n",
        "    dataset_text_field=\"text\",\n",
        "    output_dir=\"/content/ckpt\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "trainer = SFTTrainer(\n",
        "    \"facebook/opt-350m\",\n",
        "    train_dataset=dataset,\n",
        "    args=sft_config,\n",
        ")\n",
        "\n",
        "# trainer.train()  # 실제 학습할때만 사용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a3ed1d7-d403-443f-9cba-f70cd22e586c",
      "metadata": {
        "id": "6a3ed1d7-d403-443f-9cba-f70cd22e586c"
      },
      "source": [
        "## peft_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b95c9ee-80f8-4d63-a25a-0ea470e4d92c",
      "metadata": {
        "id": "6b95c9ee-80f8-4d63-a25a-0ea470e4d92c"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    \"facebook/opt-350m\",\n",
        "    train_dataset=dataset,\n",
        "    args=SFTConfig(\n",
        "        max_seq_length=512,\n",
        "        dataset_text_field=\"text\",\n",
        "        output_dir=\"/content/ckpt\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        "    peft_config=peft_config\n",
        ")\n",
        "\n",
        "# trainer.train()  # 실제 학습할때만 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86e6db38-3ef1-4582-8ace-554e93842d2b",
      "metadata": {
        "id": "86e6db38-3ef1-4582-8ace-554e93842d2b"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    \"facebook/opt-350m\",\n",
        "    train_dataset=dataset,\n",
        "    args=SFTConfig(\n",
        "        max_seq_length=512,\n",
        "        dataset_text_field=\"text\",\n",
        "        output_dir=\"/content/ckpt\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        "    model_init_kwargs={\n",
        "        \"torch_dtype\": \"auto\",\n",
        "        \"load_in_4bit\": True,\n",
        "    },\n",
        "    peft_config=peft_config,\n",
        ")\n",
        "\n",
        "# trainer.train()  # 실제 학습할때만 사용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d77337f9-36d8-4992-83e5-9033466ac11d",
      "metadata": {
        "id": "d77337f9-36d8-4992-83e5-9033466ac11d"
      },
      "source": [
        "## model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d214967f-c72f-4acd-af1d-10c86a5a955d",
      "metadata": {
        "id": "d214967f-c72f-4acd-af1d-10c86a5a955d",
        "outputId": "15fbe37a-ca89-4c61-82ca-4145fe23fcdc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ModelConfig(model_name_or_path='facebook/opt-350m', model_revision='main', torch_dtype=None, trust_remote_code=False, attn_implementation=None, use_peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, lora_task_type='CAUSAL_LM', load_in_8bit=False, load_in_4bit=True, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from trl import (\n",
        "    ModelConfig,\n",
        "    SFTTrainer,\n",
        "    get_kbit_device_map,\n",
        "    get_peft_config,\n",
        "    get_quantization_config,\n",
        ")\n",
        "\n",
        "model_config = ModelConfig(\n",
        "    model_name_or_path=\"facebook/opt-350m\",\n",
        "    load_in_4bit=True,\n",
        "    use_peft=True,\n",
        ")\n",
        "model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a68533b-4302-4c19-a1a1-53f6f32433a2",
      "metadata": {
        "id": "9a68533b-4302-4c19-a1a1-53f6f32433a2",
        "outputId": "c6c8a5d2-2c4b-417d-8104-f4b6f06c9561"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BitsAndBytesConfig {\n",
              "  \"_load_in_4bit\": true,\n",
              "  \"_load_in_8bit\": false,\n",
              "  \"bnb_4bit_compute_dtype\": \"float32\",\n",
              "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
              "  \"bnb_4bit_quant_type\": \"nf4\",\n",
              "  \"bnb_4bit_use_double_quant\": false,\n",
              "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
              "  \"llm_int8_has_fp16_weight\": false,\n",
              "  \"llm_int8_skip_modules\": null,\n",
              "  \"llm_int8_threshold\": 6.0,\n",
              "  \"load_in_4bit\": true,\n",
              "  \"load_in_8bit\": false,\n",
              "  \"quant_method\": \"bitsandbytes\"\n",
              "}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "quantization_config = get_quantization_config(model_config)\n",
        "quantization_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc9b17da-2c01-451e-b010-2322ea621a43",
      "metadata": {
        "id": "dc9b17da-2c01-451e-b010-2322ea621a43",
        "outputId": "3c2aa347-6cba-4bb4-d7e7-814e1fc773c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'': 0}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_kbit_device_map()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05dedd54-bf54-4f29-8dc5-11b6fdcce821",
      "metadata": {
        "id": "05dedd54-bf54-4f29-8dc5-11b6fdcce821",
        "outputId": "504b3118-75f2-4cdd-ac02-202d8820ef65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_config = get_peft_config(model_config)\n",
        "peft_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa248ced-71b6-4161-ad11-b8faa315342c",
      "metadata": {
        "id": "aa248ced-71b6-4161-ad11-b8faa315342c"
      },
      "outputs": [],
      "source": [
        "torch_dtype = (\n",
        "    model_config.torch_dtype\n",
        "    if model_config.torch_dtype in [\"auto\", None]\n",
        "    else getattr(torch, model_config.torch_dtype)\n",
        ")\n",
        "\n",
        "model_kwargs = dict(\n",
        "    revision=model_config.model_revision,\n",
        "    trust_remote_code=model_config.trust_remote_code,\n",
        "    attn_implementation=model_config.attn_implementation,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device_map=(\n",
        "        get_kbit_device_map()\n",
        "        if quantization_config is not None\n",
        "        else None\n",
        "    ),\n",
        "    quantization_config=quantization_config.to_dict(),\n",
        ")\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    max_seq_length=512,\n",
        "    dataset_text_field=\"text\",\n",
        "    output_dir=\"/content/ckpt\",\n",
        "    report_to=\"none\",\n",
        "    model_init_kwargs=model_kwargs,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model_config.model_name_or_path,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        "    peft_config=peft_config,\n",
        ")\n",
        "\n",
        "# trainer.train()  # 실제 학습할때만 사용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a065aa5-29bc-460d-a4b3-a7b40554e926",
      "metadata": {
        "id": "9a065aa5-29bc-460d-a4b3-a7b40554e926"
      },
      "source": [
        "## neftune_noise_alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850a00bf-d498-4aff-8875-920855f26f32",
      "metadata": {
        "id": "850a00bf-d498-4aff-8875-920855f26f32"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    \"facebook/opt-350m\",\n",
        "    train_dataset=dataset,\n",
        "    args=SFTConfig(\n",
        "        max_seq_length=512,\n",
        "        dataset_text_field=\"text\",\n",
        "        output_dir=\"/content/ckpt\",\n",
        "        report_to=\"none\",\n",
        "        neftune_noise_alpha=5,\n",
        "    ),\n",
        ")\n",
        "# trainer.train()  # 실제 학습할때만 사용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ca0099b-f7b3-4adf-acdd-2965272152e7",
      "metadata": {
        "id": "4ca0099b-f7b3-4adf-acdd-2965272152e7"
      },
      "source": [
        "# 8.5 PPO: Proximal Policy Optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ec6646-897c-41a7-b210-12dbbc1f7dab",
      "metadata": {
        "id": "31ec6646-897c-41a7-b210-12dbbc1f7dab"
      },
      "outputs": [],
      "source": [
        "# 사용법 확인 용도이므로 제대로 학습된 보상 모델 사용 x.\n",
        "# KLD 값이 음수로 떨어져 다수의 warning 메세지 발생. 이를 삭제.\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36ba0e01-19c2-4d3f-b064-b8c83b8ea2f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "20dc10569e7247338df2268d24d0f269",
            "0abc1e02283a490883cc72002cb87cfd",
            "4e9308d304c641a3be17b753d292e427",
            "50d024ade9de42829409bc4b521b6bd9",
            "4032473c18704a4093cac3d33e2a28b0",
            "6c7304bf03be4dc88639aa2efb7a5705",
            "81d18fc4fdf248fb91cba7ccfbe72821",
            "14abcfe198b7401b9310050447b100ae",
            "8a71546544794f1ca4a228476ab5b5eb",
            "356f1965218e429a8a4ff858bee51be5",
            "8eabe11ff47848dd95c2a43e471bed63",
            "7cb5166986c54e65897c4f19d181d9df",
            "646867f7053041a3a51c22fa117cfcc2",
            "0443208090a444cfbd6a544e90b00ff2",
            "ed40efbf370b473e8e744396a06cf98f",
            "e15d12ab58f64ef98926ab6fd7ac46d9",
            "ab2ccf1e854f4cefbc07bd14e37c8163",
            "4f65d333c8b54ba4af5eff90f01f9155",
            "c07c15c2c8824cbd9c6ce31719deac26",
            "b297a89f719c4ade948c9cedbbfce669",
            "2bd64a7a741f4da18d4b204a9117e11a",
            "0b41db58a9654f519c877de098c132f5",
            "db7792315d5f40099228ff6d3b776789",
            "80281db446414fec98cfdde30c9563d5",
            "75984812d9c94daeb1e1dfbd4114ff72",
            "cd6ee058acc04a158d82ef710225ec78",
            "704a61452e934f6cb6af16a4c4b88691",
            "8b9e2cc6dfcd45968a121650b65d8cce",
            "c8125be1e7df4390bbddf1bcc85273be",
            "ff4722cd2be940ff9e36500cea8b3162",
            "5b9bdebfea864deda96fde0744025406",
            "464c52c2c2dd4190be59fb04b7ef71b6",
            "9ad5332c41e24680b1137f1d70eafc5c",
            "ff5ffcb8f81b4a5f8ff8535a72ad353e",
            "dac13109c3c6484eab5b8a1852383eea",
            "b1c5a5c1b8f5417b83948ad71778e1ec",
            "e647e7d15a3747a3987d2e844f2f899a",
            "8c84abe486db4ae4a8ff6233711fff8f",
            "732f1ad7ffd842348adbf7423eaa077f",
            "155689642e544d41b56a88d4abdcdb53",
            "15f3cc0a0aa641e4a2da194c0f1bf6ca",
            "969b3fc3c6224ed28c69e3835c99f030",
            "8c79ac1f538643469fafca5199422f0d",
            "b32e653fbcbb4d4b860cb9eb495da480",
            "11d31de8566a477c811450e977a4eed4",
            "8cbef3a52e024ac68e567853ac189571",
            "389c473fe3c64e8aa0b1a44219ff1ef4",
            "4abac6a901c3481a8b5c9192bdccd4c1",
            "702b63f2f89f48b3a65d2547a3427c02",
            "71932acfe41a44dcb8e4bb93f47da0cd",
            "b8cded4b866d4f03ba042b00f8978959",
            "66d38c0640fb498dbeb1bdfe4e44214e",
            "4288ef81395d4143ba4e513184192e09",
            "7c9b54de52dc439b8187dbc33a5adfe1",
            "c571fe4ca37f4e4696a3ddfc0cf2e17d",
            "ebe7931581e242ce984dc70de3546e88",
            "272a03d1062641c7bcd365ae9627ea98",
            "5045087a52854276adaf7a6eb8e0c30e",
            "eb818034843d468b924059ec8482a1ce",
            "c699fa55ced54423a88691f640986b2b",
            "2e300c1ae06f409a9ea58daab8520d7a",
            "026b10265d464f07829a1ba2b47a8b9c",
            "aef0f88193fa4e93b514cab05221d870",
            "92bfbdeffea74ac78a17315c58642356",
            "884b649bef484f3bb8afa2353be3a797",
            "55b807dc1e8c4ccba3a7be63c17cac3b",
            "b2d115317b614702942f0098ba7efcbc",
            "6748e9c5d0e342ff8d3d7cd24a1cb7eb",
            "2dfe68dde0814b4096952e68d5a294fb",
            "40f73bb8d55f4001820f34f6179c382b",
            "70035195d01d468bbff6108e058cd2c7",
            "4699bf3c416d46a3b6179f4f27a6dd54",
            "042c0b473d4c4e6a9d0b88a7baa67d46",
            "fbc4f31c6b91435680f81169f4175552",
            "f3d2c6d2a70b4a159e6271e9b55a09a1",
            "112f6bf94860449aa19e2b8be6baf400",
            "c985033498314774a137f59f0ff9dae7",
            "7b90427bf6b0447caf046ec06f7ea451",
            "e28308b8a7ff465e9a53440f5af601ff",
            "77523f5d7ffb43f4a0ea23650a626ccb",
            "3a8e8235770745248f3fc6742336f922",
            "5beddb9d805b459aac9722963f8006ef",
            "6da29af6734b45e29d553da6dcfd0e3f",
            "ced1614bc9af4944a9c283ed8bd53bc8",
            "a151280fae384a48b5b14ae742829a0e",
            "afac6c5804c54d998b47255d6fab2dc6",
            "c94bc286a5604499b89f50105684d29b",
            "8cd4337691654876bb77e72603e27eed"
          ]
        },
        "id": "36ba0e01-19c2-4d3f-b064-b8c83b8ea2f9",
        "outputId": "1bbd9606-32b2-4de0-9400-0859631490fc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20dc10569e7247338df2268d24d0f269",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7cb5166986c54e65897c4f19d181d9df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db7792315d5f40099228ff6d3b776789",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff5ffcb8f81b4a5f8ff8535a72ad353e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11d31de8566a477c811450e977a4eed4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebe7931581e242ce984dc70de3546e88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2d115317b614702942f0098ba7efcbc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b90427bf6b0447caf046ec06f7ea451",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
        "\n",
        "config = PPOConfig(\n",
        "    model_name=\"gpt2\",\n",
        "    learning_rate=1.41e-5,\n",
        "    mini_batch_size=1,\n",
        "    batch_size=1,\n",
        ")\n",
        "model = (\n",
        "    AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "        config.model_name\n",
        "    )\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e9ecbf0-a6a9-4c84-aad5-45fb43577d94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "5bd3bc431d8a4ef08dbf1b1808d6a0e7",
            "b8f15b115edb435fada07886e208b7ec",
            "70fdf50a2e234299be74da3753a35d26",
            "3c5f4dc85d3a4eeba7ef3946ac63ef62",
            "43fa50be25764362a81f00f15e6183d2",
            "d0534c7fd7874811ab94edafcfadd2c8",
            "6d2a6a51a99e414e907a2eb7b9239c82",
            "4c35c633ec084ea39560333fe88b2d48",
            "2993ffcc53034b55aa1cc882f204c8fc",
            "7d85cbb6cafd4855bb6226b1e4486b35",
            "b8a13fee9d8f404098212936247c920e",
            "7d2ee3a43e4a4ef18d4a1e2cefcf85f3",
            "e05504c8bc1143a68c516a6b5347e245",
            "b6126e0df9784def9104a38616e60b82",
            "303b80e34d0447d3afd364a4d0cf5341",
            "e4fa4359cc39468c86ad2954b94e2694",
            "3f8b83ae154e4d7bb482c00ebd94f76b",
            "d48ecdccf666406f8db43b5f2f6d81da",
            "c3344fba8ff2453a9ed0e106780d1aa8",
            "959adbb9ce634a2cb40a23308ee50c9c",
            "7d3add3ab32249c59ea55ee6b7813765",
            "df100286bf384ccfa9ce29ff53803325",
            "359333b7229443e19bf507f10fe1ab3f",
            "b2c10e51e4e546e9acb2751d3abe45c4",
            "c1466de73add4fda82a01cd23baa96c9",
            "bfe59de8a89c48c8833b73519654927c",
            "e894b200327b4420808c7224f8d851f7",
            "ea258553604b4cbfa10d5e11bd06d79b",
            "1efb27695b984031a2a65c2d012ba91f",
            "4f2d26aad8ed4f969543cd6ed0c00301",
            "9149e58a73f54e62ad202ab18d41d358",
            "56de6e156d784aa88d845d3a8c8d288d",
            "237586d7df62471da4c2cf2bb4030345",
            "28deaf0b3f9341a8832d212b64ee25c8",
            "850995e1c77445feba2e2e7ce9a9f90b",
            "65e2fd9d9349433bb5f14fd064f0723e",
            "ebb2e58b7a4f4bf4bacd27a2a69de310",
            "139e4ff8b6e44ece83832a0de29c7516",
            "a111066f0ae64208b0ecd332292d963f",
            "094490e7cd284ddbb97c6ab3091b04dd",
            "6c851b6f95d345c7b79f4086d0f2742e",
            "fbaa9bbe97fd4bafadb968cebdc2a640",
            "e30735b2159d4220a957b70860b714fb",
            "9e8ec1c524264bde9dff9b8deb88fcc3",
            "8c2ea106da3e4adc80478284ff04ebe1",
            "da836e5bec8e4a1c84afb3dd2a64b7a6",
            "570a2039bd934cf29201dbe7b55949b4",
            "4dc2c7e12b0a4d6b88c13605d99cab45",
            "f2f17795742e46cdb44b0530fa2c4829",
            "1786521e00f14e9384354634432825c8",
            "ccf949807f5f498f998803d050b74190",
            "d17b06b4b43e4f97a0cc1c114a05a1b7",
            "8afecaf8efc34eddb81e53207f9d855c",
            "440a771d877247878b5927e71cd6ce55",
            "4626c8e3aa5943849acb5ddb2cbb2d17",
            "0471004d8e604412879da01c75618958",
            "2ab6cdb059724816882d18874cb74aa2",
            "3dd19890cef941409e609df222d772bb",
            "e3672faba99048a79865fc3407fad01a",
            "cba8c37adfcc419b81394dc2a3b241cf",
            "05d0358d11f04f2db40e1d7a5595aff7",
            "393d05c1989348699401e43e1ffc0a97",
            "3f3fc2a81314422e94343991f7a66dc2",
            "f3c6ea7dabac43e3b18f45ffc77c64a2",
            "7ba1a1913563429eafad7359354618cd",
            "598717513c4442ec8fbb51bc55c2bd18"
          ]
        },
        "id": "8e9ecbf0-a6a9-4c84-aad5-45fb43577d94",
        "outputId": "dfd20b13-b969-4838-87d7-5f4e6153fd3d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bd3bc431d8a4ef08dbf1b1808d6a0e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d2ee3a43e4a4ef18d4a1e2cefcf85f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "359333b7229443e19bf507f10fe1ab3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28deaf0b3f9341a8832d212b64ee25c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c2ea106da3e4adc80478284ff04ebe1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0471004d8e604412879da01c75618958",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "reward_model = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"lvwerra/distilbert-imdb\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af7d1cfb-7f23-4c2a-9076-cf418475e7ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235,
          "referenced_widgets": [
            "2507702803574b2685a82994c2249fc0",
            "99b33c087896425fb267ce63397dbd4f",
            "821547e60059431d936d113a13fdafd5",
            "c8ad27d006c348bd894701bc2b49657b",
            "26144f2849604bc7838930563126305d",
            "8d84767ab26244bda37bd830bce242c9",
            "efc6f9d391cf4f718c593442056001c0",
            "631ad91e1af440e99c770a0ecda55f67",
            "58a41d92fd8847b2bb31d48387d41ae5",
            "08a704e4f8984c8f8d0c781ead220edb",
            "4920397bfeb84c338d6d465bd325e8bc",
            "48bf95d41c3f43e89a2dd26ccc455157",
            "39c052128b97458685722fc917a06ccf",
            "07718b56133c4c229767298fa0ae6481",
            "62021bb220b44b54af4c8ec505e9095e",
            "0044436476bd478980c77f429b384d62",
            "c900913392004bf9871b13e882622918",
            "5f69cbd636084e2cbc02ea5f5521642e",
            "db0e89613ba24c898f764fe713f358c4",
            "ef175016d6e34bb2a767653759b1ac21",
            "a8953d8249394271a1be33975991e51c",
            "48dcbcafcffa414baead01b48a260396",
            "ead045eebdab40b49d5e242e68f00f3f",
            "d3df8367d62849b8a04765e0c4573446",
            "92fd51ab8a08445b8d1a906ccfcae585",
            "1e323dcdc70a43f483d21585bb50309c",
            "3ac2c12c400043868d8580554cecafa3",
            "2a27c59389dc4fc5bc3dad3ec04ab820",
            "5959a9ca814d490580683b0f291d55d8",
            "fb03e68d5ef04e45bd4e389c8b16f604",
            "8cf8ec8df4f24c2bbd4c7e759c21966c",
            "1806598889a5457d95fed9f80ef18e03",
            "1beb49e6d69a4c1299ec316d286baa84"
          ]
        },
        "id": "af7d1cfb-7f23-4c2a-9076-cf418475e7ae",
        "outputId": "522954f6-fa44-4a9c-e097-073f35aaeb00"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2507702803574b2685a82994c2249fc0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/1.59k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48bf95d41c3f43e89a2dd26ccc455157",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/16.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ead045eebdab40b49d5e242e68f00f3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/16 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['prompt', 'completion', 'meta'],\n",
            "    num_rows: 16\n",
            "})\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'prompt': 'Explain the moon landing to a 6 year old in a few sentences.',\n",
              " 'completion': 'People went to the moon, and they took pictures of what they saw, and sent them back to the earth so we could all see them.',\n",
              " 'meta': {'source': 'instructgpt'}}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"HuggingFaceH4/cherry_picked_prompts\",\n",
        "    split=\"train\",\n",
        ")\n",
        "print(dataset)\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7a32441-343d-459b-aece-3d86226ff132",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5d414c66d03e4ed78ae9f99bace15fae",
            "0fc458c83f8a40dab29562c26b078e99",
            "106664225512461fb8209353788c64a9",
            "9607c990100240ea834259ac4d8368ae",
            "c970f49b93994279a748f2ec9b73ed6a",
            "20b7d351922f4ee2b5808401793f8863",
            "c08f040d9a914e14a7af0988e5335687",
            "0ce3388b323a48d9acefb5e8917231fb",
            "8a2ccdb90ca94b9cae0b9132b74c7267",
            "377902276c424f1f95550f3b2bc6ee06",
            "b355f2d59542497789e770e459bc5e7a"
          ]
        },
        "id": "d7a32441-343d-459b-aece-3d86226ff132",
        "outputId": "1c17d144-bb2f-4075-ee72-0e9450f14bed"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d414c66d03e4ed78ae9f99bace15fae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def tokenize(sample):\n",
        "    sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"])\n",
        "    return sample\n",
        "\n",
        "dataset = dataset.rename_column(\"prompt\", \"query\")\n",
        "dataset = dataset.remove_columns([\"meta\", \"completion\"])\n",
        "dataset = dataset.map(tokenize, batched=False)\n",
        "dataset.set_format(type=\"torch\")\n",
        "\n",
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d158720c-78d9-4932-a57a-d0df2e39ab56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "043c116099104ae2849f80290b3249f9",
            "51c18b0d6bda4d6baf737190c34f5afb",
            "6343b28978d14e94b524a501457b47c9",
            "b16af27b5c264f8dba48c209e2454153",
            "ba0de0bb0b924a4c85d4e2a7a60480a0",
            "e2c6f521817c47f2b154230722c92947",
            "4e817b194aa845eb8e561e99f0b7e5e2",
            "430e8b87e8bd4fef898fb400b362c765",
            "d2a24972a7e74deb9b2d526ae4537a5e",
            "5faf5798414b4d7ab6f75bd49286f74a",
            "4f9f05337ee94c669a18a025efd552aa"
          ]
        },
        "id": "d158720c-78d9-4932-a57a-d0df2e39ab56",
        "outputId": "83a8c457-4489-4496-f27d-6aadf3961ce8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "043c116099104ae2849f80290b3249f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/16 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "generation_kwargs = {\n",
        "    \"max_length\": 400,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": tokenizer.eos_token_id,\n",
        "}\n",
        "\n",
        "dataset = dataset.filter(\n",
        "    lambda x: len(x[\"input_ids\"]) <= generation_kwargs[\"max_length\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd33d74-77e8-43b2-86b6-bbcd28775ac7",
      "metadata": {
        "id": "2bd33d74-77e8-43b2-86b6-bbcd28775ac7"
      },
      "outputs": [],
      "source": [
        "from trl import PPOTrainer\n",
        "\n",
        "ppo_trainer = PPOTrainer(\n",
        "    model=model,\n",
        "    config=config,\n",
        "    dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "840b488b-4a31-4ea6-b4a9-266ba04f3593",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362,
          "referenced_widgets": [
            "1f1fd26bae6e4d94b38c4a15dceebdba",
            "7dd7affbd1c64599af3011f050a57e3a",
            "7833caa1af81422b8d123c443f5736cb",
            "81013230a73344ff9f73558b361ccf13",
            "8c7fd5fd8e79448ab7441bc1b4052cfb",
            "0deb310d15ca46d5bf250ff9fc98e1cb",
            "e648d39c90304800b0906ef70fe34e19",
            "541973a7b55c4406833337733c65eb0f",
            "069ff3cd405844c59e689d06bf7aa45e",
            "7b44a7f03344462eb64e0f8fb6df63e4",
            "4b86825f0a9a48acae1a791f1e1f9e27",
            "6cc594f5b8e54511816e7601b165c67c",
            "5a7c1e8ea8114b519950d76247ed521c",
            "d0207500ad3747869f4e02bc40dbd027",
            "cd12fcedcab746aa866fea0918877e48",
            "1dc8df6b7a304b50a24b074c4e289111",
            "14912ceceb0e4c9785fa4bdf5dfebc32",
            "237583019c214cfa87716709a4f0e439",
            "2495dea0d8bd4083be9954fc9f0efa0c",
            "49f3541e85904d6aac5c188acee5a888",
            "23e4be3c7141461f9a2d5ef2166d10ca",
            "af6e32ed98c045abbe63a32de61741fd",
            "db29039c2d3940f9b9910fa91e28b143",
            "19c30be1b6ad4b0a87e655436627c035",
            "b12a9ab8024d43b0ba3e21f540ee99e8",
            "4e6a70040dc94e9c89961b508c695390",
            "74054748c2e94b78b24be44ea3aec65b",
            "83a9ad44bf1e40be89c4672bfb65fd17",
            "4b9cd8efb2304b01aee2af2f7cf3d2d8",
            "023f947564fe4cfc9e46347fa2b0bd9f",
            "42802d6208f94cf99d39308aef6156b2",
            "6081ecb0715142b094a49b0caf77a4ee",
            "fb265925789b4ca295e7302230803960",
            "210ec5422a5c4c639116e95cb39c56b0",
            "000a1a3091f04641993f3cd4bdc3f01f",
            "e4bff4334ce544deb2c137ae9e596732",
            "72a9815b1bb54e67884f1d757d82388b",
            "c9760569e26b484dba8e8907b4b38072",
            "98935ad2a69a484a9569a2920d3acb65",
            "5e749eec93c04e49a0a1343d580b7322",
            "20e969355be54e4aae242c993426460d",
            "02b08bce67bd44af998d0d0c0613c869",
            "7304487ba0624a62a2c544e4fb1afe41",
            "77a807a740b644dd91dcd264061f348e",
            "26810a1cb6fe428dba4e6d735f2fbb55",
            "88258bb106784500ac2d3872394887c6",
            "478c20d16b7d4cddad305d361f1ae6f1",
            "92e283b652c04b8f99ac32b7fcd76489",
            "c0d67edfc37e44dd835ad6c466f39353",
            "09c8c5d40fe147e98de83881ce824b4c",
            "c8e089b127a9443589c49f5e064afb46",
            "da74628616464a1cbf7a588d1016b37f",
            "f5fc57a05bf548fb92764ab9ae67bd6f",
            "ce2c67a5f7ad48068264944e84d935db",
            "dde3f47005ea42d38f2f57d643d1b199",
            "e46deefb39b6438490bc82294442cf61",
            "d9e2d0b25d77442d84df735b58bb8caf",
            "c3437b26a73147c4824518901b7055ce",
            "cda6c41a4d624fb3b85a73e54d5cb1dd",
            "b8941ed146de42178b0e358f8d46fc96",
            "bc45b7eef7c84fe39f47e4daa433d674",
            "bc6d2ff4907049d1bb6271051bf569cb",
            "b9b7637e6920449098ad0330c18e4575",
            "cefabbaf7fb448cd9d82cae09481bac4",
            "090e3b6666214682893a8200bc3e9ab8",
            "1f1506a6e47244eb95ab25603cff3c65",
            "04364a5729124e2a8c1ee171ce56fbc8",
            "d2e271b0077846bba3a288fb9de216fe",
            "499c4f8c8dce4dd1b0448263e80c4ec2",
            "bb3cedcbdf074c46b7e6b09fe1883249",
            "18f959fc3672418d9d6f0cd20fe5f0e3",
            "40080c6a0cb74dc092d0806558de6944",
            "bd5aff125fdc48b9a15a6a1e6ec26a41",
            "0a69b7d7d8094386b70920fc2769c4e2",
            "f305edddd2e54b208dc441df1e362046",
            "f6ac072598c64ae28fe0b3714bfffaa6",
            "883e3d8371bd49538d42fb8f4d2cb379",
            "0da4ddb8118b407e8e764f66b9cc1bd1",
            "715c49b05d4c46339f038dc5e0c15265",
            "7413390f9eda457db3cc5e76526bc811",
            "96b08cc8a3834b5c90a15c5b05c55da7",
            "2eb82dd3683848f7babfceeb1aabd4d7",
            "218cb9756a194843b218cfcd3ce7c186",
            "8f1d73cf3b2b42789514d2b8f23217cf",
            "ad2f1c6dd4cd4cc6b3c2de89812b1c90",
            "5d1ad3d04bf54666bc2f787d39ded225",
            "91757f18a68641a788da5455ffde1411",
            "aebda19d75a643cea8f8e317d802061a",
            "594eaf27b62247cfbcaad71ec3cd33e0",
            "4154ad097d95495f9a6b1a871db79ee3",
            "fab7cf7e67de4e9eb6b02b383de0e5fd",
            "b46647879af347e49a603412ba9348b0",
            "22678d1064cf4444817d94b3138ca69a",
            "2fba82b90c0841309c159b88af583ba1",
            "a643a3612dd447b9a0894193d8c18d71",
            "dc47e708847540e8b3a5599accecfeb4",
            "91351e924090432da6dbc397cf3baa45",
            "a393f644edf4448f88e5c01b6f53d833",
            "f510592c2a8d4135b6052a131f128f9e",
            "ecea61e4096248d5b08c47306c7e6106",
            "3549a13145b4426aaef113b911cadd19",
            "2018bae9b1384155bc5b0b1a631ca3aa",
            "f03859a6c1444e4997b09dd1dd98a57f",
            "fc45c2554de24e9281657939b8cf9313",
            "a0557441751d4e3487f4fabc1b457b00",
            "d2dac6c170eb44d0abf34668394fec6e",
            "f630f506dd65484b99271979f235c627",
            "cc062bf55e184625afc69b1305082c92",
            "d6dce9dd21f24629a37bd93368c81be2",
            "c69ad1ac937b442da03116e80a75eeb8",
            "ed8c13143d4c4efcbd70d5484c8a55e9",
            "a0a10f03bfb94148ace8cdb67f7c122d",
            "f14fa3d0ec644213ae216c4895a8a965",
            "95dbde87064845da90ca3eaa590f746f",
            "f516776e795348ebb6c08b8ba9131312",
            "5c75aa27be8a48c889a3a361bbddb99b",
            "dc74ff0bd22143ba897bac2fb0ae2b5c",
            "b15563f9936745848f09550d365f76f3",
            "39634af912de41e5ae5ecce183e48f79",
            "e03b80e1caaf44e5bbae5ec843b021d8",
            "76b32d6ec7b54b72878217c41e0cf20e"
          ]
        },
        "id": "840b488b-4a31-4ea6-b4a9-266ba04f3593",
        "outputId": "8011751f-bf89-43db-eeb3-027999948532"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f1fd26bae6e4d94b38c4a15dceebdba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6cc594f5b8e54511816e7601b165c67c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db29039c2d3940f9b9910fa91e28b143",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "210ec5422a5c4c639116e95cb39c56b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26810a1cb6fe428dba4e6d735f2fbb55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e46deefb39b6438490bc82294442cf61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04364a5729124e2a8c1ee171ce56fbc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0da4ddb8118b407e8e764f66b9cc1bd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (6528 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.Size([400])]\n",
            "[6528]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "594eaf27b62247cfbcaad71ec3cd33e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecea61e4096248d5b08c47306c7e6106",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed8c13143d4c4efcbd70d5484c8a55e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "epochs = 10\n",
        "for epoch in tqdm(range(epochs), \"epoch: \"):\n",
        "    for batch in tqdm(ppo_trainer.dataloader, leave=False):\n",
        "        query_tensors = batch[\"input_ids\"]\n",
        "\n",
        "        # Rollout: 학습할 모델로 문장 생성\n",
        "        response_tensors = ppo_trainer.generate(\n",
        "            query_tensors,\n",
        "            **generation_kwargs\n",
        "        )\n",
        "\n",
        "        batch[\"response\"] = [\n",
        "            tokenizer.decode(r.squeeze())\n",
        "            for r in response_tensors\n",
        "        ]\n",
        "\n",
        "        # Evaluate: Reward 모델로 점수 부여\n",
        "        # return_full_text 옵션 입력이 불가능하므로, response == query + gen_text\n",
        "        try:\n",
        "            pipe_outputs = reward_model(batch[\"response\"])\n",
        "        except:\n",
        "            print([r.size() for r in response_tensors])\n",
        "            print(reward_model.tokenizer(batch[\"response\"], return_length=True)[\"length\"])\n",
        "\n",
        "        rewards = [\n",
        "            torch.tensor(output[\"score\"])\n",
        "            for output in pipe_outputs\n",
        "        ]\n",
        "\n",
        "        # Optimization: ppo 학습 진행\n",
        "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "        ppo_trainer.log_stats(stats, batch, rewards)\n",
        "\n",
        "# 모델 저장\n",
        "ppo_trainer.save_pretrained(\"/content/my_ppo_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd4f05fe-ffa6-4c8f-bf38-787ad4643659",
      "metadata": {
        "id": "cd4f05fe-ffa6-4c8f-bf38-787ad4643659"
      },
      "source": [
        "# 8.6 Best of N Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a53734-522b-4f73-a684-de930a0d5591",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "5246e8d421ad434db3a177693f6e7579",
            "48ac20170b794f02a73243e07570026f",
            "19c4b4130ed3448ba7746a0ada9b2ea2",
            "4f0d32c7f2f343c087f44c9323aa023c",
            "923aedfa928e400ba975c29a03ddfa90",
            "b057d552b8274567a99f86ade296da51",
            "a3df8bd5f77f4c26a8309d964ee2867e",
            "5a10b0567da04672913419023fee49c6",
            "b5f676b495d344d1bc9ce6ff740fa806",
            "a0a3c1a6e36645ef9ed72b24078ced1f",
            "ec3ad734c6dd43a3a2f3c409a15425cf",
            "4cef70c6ae184e479bc97a44045af607",
            "a6197e21032c4668be5862a1adf186cb",
            "a5a1bbb1946e4a6192b35e6082bfa96c",
            "1d08d29e070e427dbbd18fe04fcb5ee9",
            "fe21ff41363648f8bd3dfc821954da91",
            "f4517c62ab8a4724a2c814d0d37666ad",
            "a7d7bffbb9d04ffd8a2a052f806e1e9a",
            "e46bc10869be45f49a374ca3b1333f6c",
            "947779ee3688473fbb4d89b1329af8f3",
            "9a498d0df0674a529a082ee3e84453f2",
            "149c6b49a4b74c968fd102c06c87ab21",
            "03b3d34933244d7a921edb3ca0ddc953",
            "e4ee2a1e672d47e69ba5e9b5cd9baa3e",
            "96fc9b1b38c745689cb34754fbccc4f6",
            "e3f5620063824b90a8ad7a6b3d312c2f",
            "e46ce47127784463bc62e9323e5a49d6",
            "8bdd999d812e4366957693c8dae1754a",
            "002f6dea742e4ebd9b4c7e314670dace",
            "0459ea447bc94022a1911c3db4c08084",
            "13032751c05a4708af5890f56059b340",
            "f1f48100e78a4ae2ba2731c4cd5a3c66",
            "1acb7e172e924b97a3ea9586f13178e9",
            "d085a1c836d740259c860ebbaa5151d9",
            "c689940668714ebf8417b4aab04d15a4",
            "bd82054b1f4f42aabaeee25673e73a7b",
            "5375d73310df420c8e9c5be93eb4e4d6",
            "aa2a539d128145d1867c88024c782b22",
            "598d5e78134f45ceaa56ff251b90ddba",
            "bedcca7b88bc4e9898d31ea1490e2fb7",
            "cd77b1965e5f4de58d19e20918123b7e",
            "a246baefab98449ba535c2f450aadb76",
            "60842c2b4d9a417e9abaaf6e03f58ea2",
            "789cab57cc8e4e33b8957b85c137eb80",
            "658d4d0565694006b694bd0019e3b104",
            "eb1e90a9ad584de0abeb25a69c842995",
            "3c5e832b5d1041bb86256e4934636314",
            "cce8a34ec130419a88761688b4e703ec",
            "9bcc8026f7e9487e96c5e350bf0a1537",
            "2e72621d762447de9c8ecdae0119c8f2",
            "2a9b824869b547378a2a5b67bb6cb9b0",
            "7598d619c91b4345bc9e9d3e9875d921",
            "a8c5044591104c5b95154894c0aa2ced",
            "542b7f0e6b0246f8bdd0eacca82b4edf",
            "975a8ac868c143068b55070533c65993",
            "7169d160ca944018acdba67e11faf553",
            "8a32a14ac4a945babf49b432bba1ef3b",
            "719d5b981fe647d4a3aaa22c4d37a795",
            "72936c30164b427cbc1bab901c15978f",
            "7c5f85b157de43829eb4a5e77fcb9c68",
            "2383186de7bc4af5ab1abef7fdebc9ba",
            "e1bf2a4b7ac34fd081584202b312d0b2",
            "dcea2600ada34a01aa7afe2e1fc25822",
            "0c95ce95ba9d473aa464e9026c6ce88a",
            "340b0bc597f8457ea4090c891df8e147",
            "d3e591f9a56941019b54c5305e933ac1",
            "24a813325bc94a59b046134e235fbeaf",
            "a9a28ef6ab64494bb2ea4017000aa6d7",
            "404d5846593e41da90e6e8a0289472bc",
            "872950758ef348a0a3b0778e9430e040",
            "254b04f18d434eeebd086355b135c01a",
            "384cf598a32b484199a775c087db617d",
            "c0979a84ae0a41e591f438c879df91a2",
            "207b8f10d8b8456c9cfeec73e8e18410",
            "77f8d12fb8c3464d837c4ec4129681f8",
            "aa4f809efb744e1bbc984fdb91a3dd4a",
            "868d46abe9124815a2e82ebbc30305da",
            "94b96d69cabd4f87b5f70fcfb2d96783",
            "5c5305fd45c54654af7d9382aff5cff7",
            "5bbcb631759f4f90bb51d93861db316b",
            "a099724d46d9436ba7c3ba890b360722",
            "878f866534204f47b7787a6620b3c111",
            "5307bb48f4ab4852a561b41797397b97",
            "d08072abfc0e4828ae50d78ecf8e4785",
            "725de49eae0d4144a31ce02b85d2bee1",
            "8f8b265b497b4c9dbc7fc6a968c78857",
            "454daacf26c14d3c87067167660557ee",
            "1c5d8cd81c744df496c582ab6c425fc3"
          ]
        },
        "id": "87a53734-522b-4f73-a684-de930a0d5591",
        "outputId": "b1cc16b3-14d9-444e-900a-ce18bad6d5f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5246e8d421ad434db3a177693f6e7579",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cef70c6ae184e479bc97a44045af607",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03b3d34933244d7a921edb3ca0ddc953",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d085a1c836d740259c860ebbaa5151d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "658d4d0565694006b694bd0019e3b104",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7169d160ca944018acdba67e11faf553",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24a813325bc94a59b046134e235fbeaf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94b96d69cabd4f87b5f70fcfb2d96783",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, GenerationConfig\n",
        "from trl import AutoModelForCausalLMWithValueHead\n",
        "from trl.core import LengthSampler\n",
        "from trl.extras import BestOfNSampler\n",
        "\n",
        "ref_model_name = \"gpt2\"\n",
        "reward_model_name = \"gpt2\"\n",
        "device = torch.device(\"cuda\")\n",
        "ref_model = (\n",
        "    AutoModelForCausalLMWithValueHead\n",
        "    .from_pretrained(ref_model_name)\n",
        "    .to(device)\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(ref_model_name)\n",
        "\n",
        "reward_pipe = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=reward_model_name,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "def queries_to_scores(list_of_strings):\n",
        "    return [output[\"score\"] for output in reward_pipe(list_of_strings)]\n",
        "\n",
        "best_of_n = BestOfNSampler(\n",
        "    ref_model,\n",
        "    tokenizer,\n",
        "    queries_to_scores,\n",
        "    length_sampler=LengthSampler(10, 128),\n",
        "    sample_size=5,\n",
        "    n_candidates=2,\n",
        "    generation_config=GenerationConfig(\n",
        "        min_length= -1,\n",
        "        top_k=0.0,\n",
        "        top_p= 1.0,\n",
        "        do_sample= True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b73ae3c-328e-4259-a6b0-f1d8046f07c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b73ae3c-328e-4259-a6b0-f1d8046f07c0",
        "outputId": "e56bb2fd-9abf-47e8-ecbd-8c40656a92a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "what is love? And think empire? No, I wouldn't spoil it for you.\"\n",
            "\n",
            "\"That's something the Lannisters already knew…\" \"It was suggested!\"\n",
            "\n",
            "\"Oh, quite.\" I grin now, knowing straight away that he has let his secrets be sold on.\n",
            "\n",
            "\"No,\" he says as he breathlessly leaves. \"Try going to Kreg, see how this new Rosser and Shepard\n",
            "================================================== \n",
            "\n",
            "what is love?\n",
            "\n",
            "They more like sport than art. And there is no heaven and no earth! Just pump me the water.\n",
            "\n",
            "And I must ask: Where does my love live?\"\n",
            "\n",
            "(Bram Stoker 1997)\n",
            "\n",
            "With that, Staker comes to us again. To raise you up because you're queer, because you are queer, and something was broken. Oh, what kind of breaking\n",
            "================================================== \n",
            "\n"
          ]
        }
      ],
      "source": [
        "result = best_of_n.generate(\n",
        "    tokenizer(\"what is love?\", return_tensors=\"pt\").input_ids[0],\n",
        "    device=device\n",
        ")\n",
        "\n",
        "for r in result[0]:\n",
        "    print(r)\n",
        "    print(\"=\" * 50, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30df7066-88e8-4b18-a4f1-aeb81affbab2",
      "metadata": {
        "id": "30df7066-88e8-4b18-a4f1-aeb81affbab2"
      },
      "source": [
        "# 8.7 DPO: Directi Preference Optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bafbb629-0563-47ab-ae37-ebf6bf150285",
      "metadata": {
        "id": "bafbb629-0563-47ab-ae37-ebf6bf150285"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "ref_model_name = \"gpt2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(\n",
        "    ref_model_name,\n",
        "    torch_dtype=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "137b9de5-2ad5-49a5-bf76-44fc4e51250c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "137b9de5-2ad5-49a5-bf76-44fc4e51250c",
        "outputId": "510c3c8d-fac4-47ee-9099-a0f69bef2589"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['chosen', 'rejected', 'prompt'],\n",
              "    num_rows: 10000\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"trl-internal-testing/hh-rlhf-trl-style\",\n",
        "    split=\"train[:10000]\",\n",
        ")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3135c81-ec72-4291-a642-643ee4c3b3e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265,
          "referenced_widgets": [
            "f2d4157fb23544a88288920e983ab56c",
            "fb1893b1755447068480a9f21798c4c9",
            "d84273ed4a5749d6bf11b93961e2dd5f",
            "bccdcc9ce3944bc98cab843de1cda680",
            "47079129479f4d9fa0ad1750ffd7d11d",
            "09ce3a8b43024c719733a970feaa62d0",
            "c86dcc48a93241d394daf9877c4ceb93",
            "8af47a8fd7f94a08b7f01ce610546244",
            "1fa0c3db6bc344399f24392b9c6e8e93",
            "aef2ca3ca5d54d0cbd8510caa1519cb6",
            "d86899c5801342449115dbaddd103c72"
          ]
        },
        "id": "e3135c81-ec72-4291-a642-643ee4c3b3e9",
        "outputId": "61a964fc-2921-4adb-a388-76999824a2bd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2d4157fb23544a88288920e983ab56c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user: What are some cuss words in english?\n",
            "\n",
            "assistant: Here’s an incomplete list.\n",
            "\n",
            "Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cum, cumbucket, fucknugget, butthole, poop, jackass, cocksucker, asshole, goddamn, piss, sperm, blow, wank, jism, cum-sucking, masturbate, faggot, queer, jizz, jizz-licking, prostitute, slut, cheater, fornicator, floozy, wetback, Mexican, Hispanic, sodomite, midget, mama’s boy, faggot, pervert, queer, scumbag, bitch,\n",
            "\n",
            "user: What's your favorite one?\n",
            "\n",
            "assistant: I haven't even thought about it.\n",
            "\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "if tokenizer.chat_template is None:\n",
        "    tokenizer.chat_template = (\n",
        "        \"{% for message in messages %}\"\n",
        "        \"{{message['role'] + ': ' + message['content'] + '\\n\\n'}}\"\n",
        "        \"{% endfor %}{{ eos_token }}\"\n",
        "    )\n",
        "\n",
        "def process(row):\n",
        "    row[\"chosen\"] = tokenizer.apply_chat_template(\n",
        "        row[\"chosen\"], tokenize=False\n",
        "    )\n",
        "    row[\"rejected\"] = tokenizer.apply_chat_template(\n",
        "        row[\"rejected\"], tokenize=False\n",
        "    )\n",
        "    return row\n",
        "\n",
        "dataset = dataset.map(\n",
        "    process,\n",
        "    num_proc=2,\n",
        "    load_from_cache_file=False,\n",
        ")\n",
        "\n",
        "print(dataset[0][\"chosen\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce40d91c-11ac-42e7-b6d5-db6d30d3f47f",
      "metadata": {
        "id": "ce40d91c-11ac-42e7-b6d5-db6d30d3f47f"
      },
      "outputs": [],
      "source": [
        "from trl import DPOConfig, DPOTrainer\n",
        "\n",
        "args = DPOConfig(\n",
        "    beta=0.1,\n",
        "    max_length=512,\n",
        "    max_prompt_length=512,\n",
        "    dataset_num_proc=2,\n",
        "    remove_unused_columns=False,\n",
        "    output_dir=\"/content/ckpt\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model,\n",
        "    ref_model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    args=args,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d7c0149-1111-4225-aca4-a3e47efcf6c5",
      "metadata": {
        "id": "8d7c0149-1111-4225-aca4-a3e47efcf6c5"
      },
      "source": [
        "# 8.8 KTO: Kahneman-Tversky Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92c203c2-8312-4d0c-bfdd-e33a9a433db5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92c203c2-8312-4d0c-bfdd-e33a9a433db5",
        "outputId": "ba7bdfe5-7b4a-4d65-d5d3-758ff4ba24aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"facebook/opt-350m\"\n",
        "ref_model_name = \"facebook/opt-350m\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(ref_model_name, torch_dtype=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "667bfd19-05b1-4194-874d-0adfe4bc0bd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "667bfd19-05b1-4194-874d-0adfe4bc0bd6",
        "outputId": "f6358b27-d169-454b-b6ff-2219dfa5144a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'completion', 'label'],\n",
              "    num_rows: 13500\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"trl-lib/kto-mix-14k\", split=\"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec5124d-3fee-48c0-bc8c-7d0c6ae1ca1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "c4b0750d1a2c4577b8eb8a72c232acf2",
            "4014ad6f4c13493d878cbdd1d72d3445",
            "045910d08b5f4accacaf6e62f626d53b",
            "f609121af9f748d69ef57b04b4015b6d",
            "e92be2b72334408ab47e5adc3b6931de",
            "4d7cae7903db44e8ae95c78bf0c3c8d6",
            "a899929aec394779a190e45ae4ff2f95",
            "b525384ad9b941019867078117b7b727",
            "5b2b53cc214f4df281cb950aadec1dae",
            "bd80baed04af4f1bb15b2c73959308ca",
            "006beace95c2464a8f4a6424d10f36ad"
          ]
        },
        "id": "dec5124d-3fee-48c0-bc8c-7d0c6ae1ca1b",
        "outputId": "3747fc5e-dbfa-4068-c92c-e473b286164a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4b0750d1a2c4577b8eb8a72c232acf2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/13500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n",
            "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Yes, the information you found on Google is correct. Julio César Chávez holds several records related to world title defenses and victories, and he is considered one of the greatest boxers in history. Here is a detailed answer to your question:\n",
            "\n",
            "Julio César Chávez was born on July 12, 1962, in Ciudad Obregón, Sonora, Mexico. He began boxing at a young age and quickly made a name for himself in the sport, winning his first world title in 1984 when he defeated Mario Miranda for the WBC super featherweight championship.\n",
            "\n",
            "Over the course of his career, Chávez would go on to hold titles in three different weight classes (super featherweight, lightweight, and junior welterweight) and defend his titles a record 27 times, including 21 times by knockout. This record for most successful consecutive defenses of world titles has never been broken, and it is a testament to Chávez's skill, determination, and durability as a fighter.\n",
            "\n",
            "In addition to his record for most successful title defenses, Chávez also holds the records for most title fights (37) and most title-fight victories (31). These records are also unbroken and demonstrate Chávez's consistent success and dominance in the ring.\n",
            "\n",
            "Chávez's impressive record and achievements have earned him a place among the greatest boxers of all time, and he is often compared to other boxing legends such as Joe Louis, who holds the record for most title defenses won by knockout (23). While Chávez did not quite match Louis's record for most title defenses won by knockout, he is still widely regarded as one of the greatest boxers in history and a true legend of the sport.\n",
            "\n",
            "In conclusion, the information you found on Google is correct: Julio César Chávez holds several records related to world title defenses and victories, and he is considered one of the greatest boxers in history. His impressive record and achievements have earned him a place among the greatest boxers of all time, and he will always be remembered as a true legend of the sport.</s>\n"
          ]
        }
      ],
      "source": [
        "def process(row):\n",
        "    row[\"prompt\"] = tokenizer.apply_chat_template(\n",
        "        row[\"prompt\"], tokenize=False\n",
        "    )\n",
        "    row[\"completion\"] = tokenizer.apply_chat_template(\n",
        "        row[\"completion\"], tokenize=False\n",
        "    )\n",
        "    return row\n",
        "\n",
        "dataset = dataset.map(\n",
        "    process,\n",
        "    num_proc=2,\n",
        "    load_from_cache_file=False,\n",
        ")\n",
        "print(dataset[0][\"completion\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9db4d43-f542-48a8-811a-864cdd812bc9",
      "metadata": {
        "id": "f9db4d43-f542-48a8-811a-864cdd812bc9"
      },
      "outputs": [],
      "source": [
        "from trl import KTOTrainer, KTOConfig\n",
        "\n",
        "args = KTOConfig(\n",
        "    logging_dir=\"/content/logs\",\n",
        "    output_dir=\"/content/ckpt\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=5e-5,\n",
        "    optim=\"adamw_torch\",\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\",\n",
        "\n",
        "    max_length=512,\n",
        "    max_prompt_length=512,\n",
        "    remove_unused_columns=False,\n",
        "    dataset_num_proc=2,\n",
        "\n",
        "    beta=0.1,\n",
        "    desirable_weight=1.0,\n",
        "    undesirable_weight=1.0,\n",
        ")\n",
        "\n",
        "trainer = KTOTrainer(\n",
        "    model,\n",
        "    ref_model,\n",
        "    args=args,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15acec3a-8f45-4971-8fa1-23922df7d55a",
      "metadata": {
        "id": "15acec3a-8f45-4971-8fa1-23922df7d55a"
      },
      "source": [
        "# 8.9 CPO: Contrastive Preference Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bccba57-34fb-4c38-9391-ce039faad2d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bccba57-34fb-4c38-9391-ce039faad2d1",
        "outputId": "68e98b7f-607d-4164-9899-9e415cfb1cc2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "778b9d0a-f47e-4f0a-a927-6ac63b9a667c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "778b9d0a-f47e-4f0a-a927-6ac63b9a667c",
        "outputId": "658ecb7f-3766-42ff-c2c0-ee12258e72e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['chosen', 'rejected', 'prompt'],\n",
              "    num_rows: 10000\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"trl-internal-testing/hh-rlhf-trl-style\",\n",
        "    split=\"train[:10000]\",\n",
        ")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45127d14-aece-4131-8a5c-9a032ecc4e0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "bafd2adc1e724bea825cbdebb5f7058b",
            "4deec2ebc8cf471c86d6b2e684a176e4",
            "d567ee9c613b47628f0464e8f8956e01",
            "f0bd22e892ac4f5a888a0ff1509ff463",
            "2e56dd94614940deb53eaaf2469e0cbd",
            "3a0171827436484081dfc77a09ef8e27",
            "16697538236b4aaeb16f8582656d98ea",
            "de40deea0fdf4844a852dad923944019",
            "f17efd51e15a4827bd72bc0a268bb36e",
            "d48636fa537d4cd699ca8ec763b6da09",
            "39cd4d26b4ec425fb11af7b0eda80f75"
          ]
        },
        "id": "45127d14-aece-4131-8a5c-9a032ecc4e0a",
        "outputId": "7d10eb8b-f68a-446d-9a81-04c81428b3b4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bafd2adc1e724bea825cbdebb5f7058b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "if tokenizer.chat_template is None:\n",
        "    tokenizer.chat_template = (\n",
        "        \"{% for message in messages %}\"\n",
        "        \"{{message['role'] + ': ' + message['content'] + '\\n\\n'}}\"\n",
        "        \"{% endfor %}{{ eos_token }}\"\n",
        "    )\n",
        "\n",
        "def process(row):\n",
        "    row[\"chosen\"] = tokenizer.apply_chat_template(\n",
        "        row[\"chosen\"], tokenize=False\n",
        "    )\n",
        "    row[\"rejected\"] = tokenizer.apply_chat_template(\n",
        "        row[\"rejected\"], tokenize=False\n",
        "    )\n",
        "    return row\n",
        "\n",
        "dataset = dataset.map(\n",
        "    process,\n",
        "    num_proc=2,\n",
        "    load_from_cache_file=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89188609-b53a-46c3-95a2-f2709606d979",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "113cc079d7bd4e05945b0815513b7000",
            "3b2619d24f194fd3a0387b8a136764ce",
            "82c96bf8ef004f00a08626880a49eabb",
            "a2317f2fb240499f94eba712e3315842",
            "5686bb2f652a45efb6bb977f64f3366a",
            "3ad97a2791c948199d21af88e4821284",
            "55fad52814424d659c15c38fb7652a30",
            "0143cca0f9a047cda821ffb32e98beba",
            "fc69d60156cd4e1e87720b698520542b",
            "b969cee803714d47be83894ae693ed4e",
            "1e9f241e19e04a35b482494846beca39"
          ]
        },
        "id": "89188609-b53a-46c3-95a2-f2709606d979",
        "outputId": "210bb23d-9aa9-4ba0-bd52-e987948704d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "113cc079d7bd4e05945b0815513b7000",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1047 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='204' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 204/2500 02:25 < 27:33, 1.39 it/s, Epoch 0.08/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>5.699500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>4.772200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2cd93ec198e0>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1885\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1886\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2177\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2179\u001b[0m                 \u001b[0mtotal_batched_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0;31m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                     \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m                 \u001b[0mnext_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mskip_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         return type(tensor)(\n\u001b[0;32m--> 183\u001b[0;31m             {\n\u001b[0m\u001b[1;32m    184\u001b[0m                 \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskip_keys\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    182\u001b[0m         return type(tensor)(\n\u001b[1;32m    183\u001b[0m             {\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskip_keys\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             }\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xpu:0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# .to() doesn't accept non_blocking as kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from trl import CPOConfig, CPOTrainer\n",
        "\n",
        "args = CPOConfig(\n",
        "    logging_dir=\"/content/logs\",\n",
        "    output_dir=\"/content/ckpt\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=5e-5,\n",
        "    optim=\"adamw_torch\",\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\",\n",
        "\n",
        "    max_length=512,\n",
        "    max_prompt_length=512,\n",
        "    dataset_num_proc=2,\n",
        "    remove_unused_columns=False,\n",
        "    beta=0.1,\n",
        ")\n",
        "\n",
        "trainer = CPOTrainer(\n",
        "    model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8836e9af-12e9-4342-8e4e-7098aed65772",
      "metadata": {
        "id": "8836e9af-12e9-4342-8e4e-7098aed65772"
      },
      "source": [
        "# 8.10 ORPO: Odds Ratio Preference Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aeea492-6448-461d-b1a3-7f868afb5a97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aeea492-6448-461d-b1a3-7f868afb5a97",
        "outputId": "f33424c1-69e4-48a4-8c47-183c925dc776"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"facebook/opt-350m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e891864-89a1-4b27-afee-5b3b43e86a2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e891864-89a1-4b27-afee-5b3b43e86a2c",
        "outputId": "885134a3-9d65-4316-dfe4-209260be8e89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['chosen', 'rejected', 'prompt'],\n",
              "    num_rows: 10000\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"trl-internal-testing/hh-rlhf-trl-style\",\n",
        "    split=\"train[:10000]\",\n",
        ")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7777ed63-e2d4-4af5-a2d4-b3c5b6c80753",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c3f66a74bc9c46a29c38cd9e77b2a629",
            "8c312a473a3b418584358b02e52f0e73",
            "7bbe5590aa90499ead89d945da70a5d2",
            "d58a8ae9b4904386b00c172c6e3d9a6e",
            "48993e019c724ab19545739c71cf504d",
            "ebbd3789b1724f4bb868289bc3d176be",
            "d19cc8d1ce3d4d04be3c0b7e6e4e8c92",
            "e3dbd4591dfb4d8b8ce8a9c88142bae4",
            "392be72a3c394a5a9394fbf6fac42bb0",
            "5bb95df8287e434e87d2f3cf3d0882e5",
            "529c76c0e68d4c3c8360c6dc5057448a"
          ]
        },
        "id": "7777ed63-e2d4-4af5-a2d4-b3c5b6c80753",
        "outputId": "30dd7d31-8550-4c91-d736-113b2366e865"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3f66a74bc9c46a29c38cd9e77b2a629",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "if tokenizer.chat_template is None:\n",
        "    tokenizer.chat_template = (\n",
        "        \"{% for message in messages %}\"\n",
        "        \"{{message['role'] + ': ' + message['content'] + '\\n\\n'}}\"\n",
        "        \"{% endfor %}{{ eos_token }}\"\n",
        "    )\n",
        "\n",
        "def process(row):\n",
        "    row[\"chosen\"] = tokenizer.apply_chat_template(\n",
        "        row[\"chosen\"], tokenize=False\n",
        "    )\n",
        "    row[\"rejected\"] = tokenizer.apply_chat_template(\n",
        "        row[\"rejected\"], tokenize=False\n",
        "    )\n",
        "    return row\n",
        "\n",
        "dataset = dataset.map(\n",
        "    process,\n",
        "    num_proc=2,\n",
        "    load_from_cache_file=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8c349dc-cd07-49be-8dde-863f197f5a1d",
      "metadata": {
        "id": "d8c349dc-cd07-49be-8dde-863f197f5a1d"
      },
      "outputs": [],
      "source": [
        "from trl import ORPOConfig, ORPOTrainer\n",
        "\n",
        "args = ORPOConfig(\n",
        "    logging_dir=\"/content/logs\",\n",
        "    output_dir=\"/content/ckpt\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=5e-5,\n",
        "    optim=\"adamw_torch\",\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\",\n",
        "\n",
        "    max_length=512,\n",
        "    max_prompt_length=512,\n",
        "    dataset_num_proc=2,\n",
        "    remove_unused_columns=False,\n",
        "\n",
        "    beta=0.1,\n",
        ")\n",
        "\n",
        "trainer = ORPOTrainer(\n",
        "    model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a805af4a-0ed8-472e-9efb-6904a4a45233",
      "metadata": {
        "id": "a805af4a-0ed8-472e-9efb-6904a4a45233"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5496dae6-0c2d-46c3-aee5-af396d044b7b",
        "15a35476-651a-40f6-869c-e8151aa98afa",
        "4ca0099b-f7b3-4adf-acdd-2965272152e7",
        "cd4f05fe-ffa6-4c8f-bf38-787ad4643659",
        "30df7066-88e8-4b18-a4f1-aeb81affbab2",
        "8d7c0149-1111-4225-aca4-a3e47efcf6c5",
        "15acec3a-8f45-4971-8fa1-23922df7d55a",
        "8836e9af-12e9-4342-8e4e-7098aed65772"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
