{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZdvwljjpXs6",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-G-jhIfpW4_",
        "outputId": "28fef67e-e51d-476c-831f-91e37aca2fa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==2.20.0\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate==0.4.0\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting scikit-learn==1.4.2\n",
            "  Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (17.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.20.0)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.20.0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (4.67.1)\n",
            "Collecting xxhash (from datasets==2.20.0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets==2.20.0)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (6.0.2)\n",
            "Collecting responses<0.19 (from evaluate==0.4.0)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2024.12.14)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.20.0)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.17.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, fsspec, dill, scikit-learn, responses, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 evaluate-0.4.0 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-hotfix-0.6 responses-0.18.0 scikit-learn-1.4.2 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \\\n",
        "     datasets==2.20.0 \\\n",
        "     evaluate==0.4.0 \\\n",
        "     scikit-learn==1.4.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln6Iv7fF0LMK"
      },
      "source": [
        "# 5.2.2 Sequence Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0AbIn2bBS18"
      },
      "source": [
        "### 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "28f5a739649e4c779a3626d13779f6ab",
            "e9c768ea94b7489e89f0dacc8273e5d4",
            "82de1fc2d6c644fb89fed5cc80c85f5e",
            "684502baa3bd4837824bcc9ccac2af57",
            "5700c4414f0d4f94b0e364df4f407c0b",
            "3fd9f32359e34967a2a47837ea766a99",
            "8fc612028f0c4a0ba57dce7f7ce81e60",
            "3337ca76fd7d4069ae3d105303aa34e6",
            "ba67c05c47a24afd94a90514cb0f0347",
            "f27f134a17114e4dac5bb19cba40ca69",
            "65ffa477df4b4a699b6c1c544f9ab99d",
            "7c939a45a5264a35849996bb46a9bcdb",
            "cee3faa301bf4b67951642994235c515",
            "ba78a153ef9a4d4c97fe5f5d273a61f8",
            "e88fbf46c0254e6a94345d0992730e4f",
            "187603dacea8419dbeb2d2cd3578bce3",
            "bc0e324608ad46f6a989ed118fc6d6cd",
            "47cf949b79794f6e9cd81ca61a9d6ebb",
            "53be8baad3ae4690ba603ae38b232748",
            "dfa642be82a6496f829e30db159a7ebe",
            "3ea693abfa6c4df5bec1b557e1ca78a0",
            "fce2ae7e71214698a50d6c87ddcf4f95",
            "d1f527c714034f8c9dc556a4fbccfb28",
            "179caced9d3b48f89306c0dc7b16c1d0",
            "446a3e586180452aab41db15dd78aed0",
            "e0aecb53e67144b7a3ccb77dad2bdb77",
            "c9e9dbce76094344a9500e88fdbbb3ed",
            "f2a6f521f699440d9a89cbb86947efd0",
            "272cbd64f44e45d4b8004a9cd6fe7e89",
            "5678307f47fd401993d10119368e7579",
            "27188cabe31d4542b67776a4299740e0",
            "0b85a167de7740e49c18bfbac7aa481f",
            "057fa4e2f35e42ef98f8b480fa78825e",
            "ae62e408d4184a1db7b0c23ecc9694a8",
            "798908d7ca324480bcd2ea54fce25a11",
            "ce19c831aabb4efe93362234286f102f",
            "a0e4fff40c8d4e7b99b3ccf36f8d4337",
            "f7aba11b968f40febb4df024d9b23ab1",
            "ff6ae9fe4e2246789a4a279a3e5b789b",
            "a7fdc21f35914a51ad2e9efb53725660",
            "8e3865eb102343d998d92a7b4fef9733",
            "7b41588808414b9dbd27d120054b8702",
            "545047f3bea54a429fe12a635585f9a4",
            "d024170d31af4f7fbd9368cdda19a5e6",
            "aa1378456fbc4041a3b585b04ac06bbd",
            "6dd8d4d9f15f49cc9194eb9d62191732",
            "881bda3924ad44ec8fe38176ef926f08",
            "c50c28b6983b4796a19bf88453e7894c",
            "92684fd5dcb541108796e0fb8e24ae0f",
            "b029c04f65ae4daca7a830990e2827c7",
            "b85b438ce8994a07ad6503e7b02d9ea2",
            "fcf2bfe6f0e84b31bab5004df916ff05",
            "c6a4031fb87c47d798764ec08791a21e",
            "fcf280c64ad34d398c33df48a8065cef",
            "332a1491c0cc4218b3abf1e33c32e9b9",
            "607c2ff50b624af3a755e5699810907c",
            "6fabec9a6a6c4a83942af0220423f5a0",
            "3df01c974c634860a1cafa117c124e1a",
            "d3bc1c970fa64bb7bba1f354980b93c0",
            "602a01331333494e9e14e79709b22afa",
            "bbbb8a54a45e4573b26dfe7260622c49",
            "aff16d8e5b384c8cbc3a3cd1c25ec9df",
            "e33763d572c7478f8961f8d6299e7266",
            "9739d45d40784f6fb745b0c0ca1f5f2d",
            "a2d9f17e72944f99b050bc9cd2668189",
            "90031a3bc01a42c9b2061353512430dc"
          ]
        },
        "id": "kYxntMr3zxd7",
        "outputId": "71cd1915-1713-4bb3-9327-70761d3fe4a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28f5a739649e4c779a3626d13779f6ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/425 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c939a45a5264a35849996bb46a9bcdb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1f527c714034f8c9dc556a4fbccfb28"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae62e408d4184a1db7b0c23ecc9694a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa1378456fbc4041a3b585b04ac06bbd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "607c2ff50b624af3a755e5699810907c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"klue/bert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo211sqvyNB3",
        "outputId": "9b544401-ac3f-40d8-e003-90f2b41c28a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'LABEL_0', 1: 'LABEL_1'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "model.config.id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "U8k73qoNxt4Y",
        "outputId": "27ec9723-6c8d-4ac7-db8b-d608dc26fbc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LABEL_1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "inputs = tokenizer(\"안녕? 내 강아지는 귀여워.\", return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "predicted_class_id = logits.argmax().item()\n",
        "model.config.id2label[predicted_class_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQOAouILA7bE"
      },
      "source": [
        "### 데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252,
          "referenced_widgets": [
            "1e71bf6172d04ac387073d1657306fc9",
            "fa3a24d12e7c4ab4902fa27837430b93",
            "1bea3ae7617d4c2c909c6af83c5724b1",
            "42ced01820574a67a8bb36192eb75361",
            "bd3d7f057bbc4aea972243f1a40979af",
            "683901f46d9a4f4f9744915d323c61ad",
            "1f037f5f6be14d2ba2cdfdfe50e7ee12",
            "ad8c9af6dd744262a130bd2f36d95283",
            "cea418e308e84657b6882b87a3b8befb",
            "099da02d7907498dbbbe0cf792756970",
            "78869e9dbed24e249020b9dc7d7b6c44",
            "05a26e0ac25348eeb84326dfeab0eb48",
            "943c410852d141a8a11308cc01399c4c",
            "60995d65c8154f1382919843bc1d072e",
            "95f97e3900494a4f9da318cc723d097a",
            "3299e827fc6f48b185c5221d2ac20534",
            "159a55539cd549349a30d247b2f9e306",
            "9458e803135f479891e3b26f2dbaa0fd",
            "fa6b80c381524a1db1801ea434459234",
            "90f7e3a68dc64b1bb4a347680bdbb0a6",
            "51ebe8a2bc04493db2d28ce2934aee94",
            "c3d74c0dc08d49bfab180fb12b1da37d",
            "816e13f5f7864d3e82607a846494f724",
            "0bca750c5a164efe86bbc114db8da2eb",
            "397fc2f5423b4ab4b16973982c2dd97c",
            "c88c2c5b8f2d49c09178a5dd094cf87b",
            "6e94f7d30a354170b68de60bd5fafc4f",
            "74f5d73303de42348d28a36d3a8d0b1d",
            "7f4754334da74f578a3ea705799fab8a",
            "54e4bbacd0c646bfa8bfde288d7ee8e4",
            "acff3d8e9ba0414ba351f6fd0ada346f",
            "b8072971797048ec8cb42d19bc97a3fd",
            "19a4437bfcc6465a8a4148b75af7e401",
            "727abf653415498b95f6c66b08234dc3",
            "82484547aaba43279d0e495f34d2fb8c",
            "6ad0906ea4eb48459567a7381c3b7638",
            "f7702a6a43a14f0ea7106e22ff6c3781",
            "7dd9236c460a4bc2a84f2996b39e0977",
            "6c5922eec4784663b6fba7a52fd6b44e",
            "7885d1f8618248648767d4aecaff541b",
            "2cbe094ba9674ccba0a6412f80f3c040",
            "6c7d6ab2a23f4202ae20d9476147a23d",
            "2504bdda98264886a624bec67c0cb8fb",
            "05eda8f6c3a240cd88b2522d9c9b65ee",
            "05ab1aa049154cc4bc24b9cf82c0586a",
            "a9e127b690454512b797f47a25882863",
            "7e0536223b8f4e57a83c8ed457593ddb",
            "afa5bc2dc8cd4f76a7a4e531944ba4d1",
            "910880d1654043c98cb495e89d38b533",
            "4e38fb66917c40a4a00eb0b681f13191",
            "1bfb8e5f3cb545c6b101b978cdec58fc",
            "680c8530103c45cda2cca85bc5707fec",
            "6c93bfafa4ba4effa217cc0d3bd5677c",
            "0232ebed945a45f6917c4396b2e61141",
            "98cde60be2854ad28d200c712761bce6"
          ]
        },
        "id": "sfyNrDRtoIHF",
        "outputId": "29059195-3115-43bc-d8c8-1d1535698bda"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/22.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e71bf6172d04ac387073d1657306fc9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.52M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05a26e0ac25348eeb84326dfeab0eb48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/68.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "816e13f5f7864d3e82607a846494f724"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/11668 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "727abf653415498b95f6c66b08234dc3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/519 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05ab1aa049154cc4bc24b9cf82c0586a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['guid', 'source', 'sentence1', 'sentence2', 'labels'],\n",
              "    num_rows: 11668\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"klue\", \"sts\")\n",
        "dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "019ec738707f4b349cd92ec75afbbb9a",
            "276d8a5de26f4a9d88b280083cf50494",
            "55bc1362ea9044a0be22fab7cc28b776",
            "f946607bb8a24d79840d48e9ff806936",
            "9f1eadcaa06a45c387b095d4c8dd77c8",
            "b40365dc98784a2c9b9c287fdc9abc32",
            "3f1fc69d36cf4333b9911b2bfa9ada28",
            "fdd2e4efd65c49be94775300caebf742",
            "374aa5c76842436fba3f1421d57835f6",
            "89751fafcc014ebe8982abe91ba5ca50",
            "e37be8dff6a4413e8888fe58a45f122a",
            "7246bcf0bbf7442dab858f6ecc293c43",
            "b68f40194c8445a69e265fb56e430079",
            "02ca10fc118f4f44b7f3aa332d2a82f8",
            "37bb64c162be41e7a40c95fcba253b74",
            "56849aef2fdd479d8e51b9df164a3faf",
            "60de7f02957e4fa8b18262cd441f8270",
            "29c64fa037134866a5a35fb94c83b175",
            "f620fca9d8034e169466c21627da09d2",
            "73f7d6e4fb0a4034aa09f6f46593085b",
            "286882f8a12b46c78db71facfb7e0776",
            "620568494edc413580f51ec944f2700c"
          ]
        },
        "id": "JnSvkKikEABa",
        "outputId": "8040ddf7-b04f-4a9e-de97-a8e987acf507"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/11668 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "019ec738707f4b349cd92ec75afbbb9a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/519 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7246bcf0bbf7442dab858f6ecc293c43"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def process_data(batch):\n",
        "  result = tokenizer(batch[\"sentence1\"], text_pair=batch[\"sentence2\"])\n",
        "  result[\"labels\"] = [x[\"binary-label\"] for x in batch[\"labels\"]]\n",
        "  return result\n",
        "\n",
        "dataset = dataset.map(\n",
        "    process_data,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qj_7WGWgGD_p"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "collator = DataCollatorWithPadding(tokenizer)\n",
        "batch = collator([dataset[\"train\"][i] for i in range(10)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgpdxSFe_UmZ",
        "outputId": "26ab8687-ace8-4eca-8b00-7ee2424fad31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0214, -0.1048],\n",
              "        [-0.3059, -0.1155],\n",
              "        [ 0.2583,  0.2576],\n",
              "        [-0.1867,  0.3904],\n",
              "        [ 0.1820, -0.1036],\n",
              "        [-0.3989, -0.2500],\n",
              "        [ 0.0538,  0.1395],\n",
              "        [-0.1382,  0.2030],\n",
              "        [-0.3700, -0.1376],\n",
              "        [ 0.1899, -0.0550]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  logits = model(**batch).logits\n",
        "\n",
        "logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsEoysk1Fls1"
      },
      "source": [
        "### 평가 지표"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t3Y1AZ8AbEF",
        "outputId": "5f519afe-d39c-403a-8dab-f4f8aaad0074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 1 0 1 1 1 1 0]\n",
            "[1 0 0 0 1 0 1 0 0 1]\n"
          ]
        }
      ],
      "source": [
        "pred_labels = logits.argmax(dim=1).cpu().numpy()\n",
        "true_labels = batch[\"labels\"].numpy()\n",
        "print(pred_labels)\n",
        "print(true_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68,
          "referenced_widgets": [
            "2b2edb209ec54c6387452e96f4807562",
            "5d2c3f9151a840a6a773d3fe23c6065a",
            "67d2d43885524a5ebc6ad4882c888cbb",
            "e52747f3b15a449b894c6de36246b5e0",
            "ce3590a010bf4487964ca2220cbfca47",
            "e5f18b58f619432c919c45f701ed577e",
            "097856d56ef4478baae74e87db79ae87",
            "dd48c7293d5d40298aa61657548a8eb6",
            "485e7594a5244fc585ba9c5661451a2d",
            "30c6591789324289a7ab7375a9b23585",
            "eba2063af49f4767a8f8f16e30b74a4d"
          ]
        },
        "id": "GUkhWekpHgck",
        "outputId": "e6f1362b-880c-4839-d51a-8e3f56f1ebd2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b2edb209ec54c6387452e96f4807562"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'f1': 0.2}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "f1 = evaluate.load(\"f1\")\n",
        "f1.compute(predictions=pred_labels, references=true_labels, average=\"micro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7UybhBNDfon"
      },
      "source": [
        "### 회귀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycJ1r0gwDdwr",
        "outputId": "d23bd2bc-6445-4dad-b11a-5190defac87b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSdpaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, BertForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=1)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26n53XKBk1c7",
        "outputId": "8b1a041c-fee9-491c-fb02-8d0073d20ce9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2084],\n",
              "        [-0.0209],\n",
              "        [-0.3917],\n",
              "        [-0.5131],\n",
              "        [-0.1470],\n",
              "        [ 0.1152],\n",
              "        [-0.2188],\n",
              "        [ 0.3465],\n",
              "        [-0.1856],\n",
              "        [-0.1182]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  logits = model(**batch).logits\n",
        "\n",
        "logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLWgmAvZR-dV"
      },
      "source": [
        "# 5.2.3 Multiple Choice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_pnL8UOnv7j"
      },
      "source": [
        "### 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oMITeZsk3kX",
        "outputId": "c374cde4-e3d9-44a1-8e3e-50a9d3904e46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMultipleChoice(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForMultipleChoice\n",
        "\n",
        "model_name = \"klue/bert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMultipleChoice.from_pretrained(model_name)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qbV1dS8a6Bw"
      },
      "source": [
        "### 데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152,
          "referenced_widgets": [
            "a987bfff0cf04819ad2394cd2ebf8d36",
            "9308464cff964a7794d8b4e63d3f9a6b",
            "cac3bb66d7a64d42aa7e9db05d1609c3",
            "e9ddf73448964426b9eb71e90df35d27",
            "54fecec3f6694954ac444b7f77d19fd4",
            "9639b4e0b8e6402db7d4906c72add05f",
            "3bfbad1198404b9ca64a339f557b3990",
            "9bcb9d16ed174455903b30f145039d12",
            "b4ba455c36c5469db3bc6fedfdf3640a",
            "f0221c47e7654ae5ae507a96e68fcd0a",
            "817eec2c70b240aca57c40b339756baf",
            "71207c2b00f444459ff5912bac0a1adc",
            "15b36d4afe064c37b65e437575fc3b57",
            "e2eba0ec6c46446589d47abdb8ed697b",
            "9614fa4aafee4380a3a0a1d67a15c54d",
            "1773d99767e645c9a2ae87df52f0172f",
            "94214d63dd7d4ab4bfd9896e9b9c9d58",
            "cc8ad3b3014848e28c94ff6223960b30",
            "9438803de24f4d58a02c0f6cd17d7ed9",
            "29961d5f50aa4d619f24f00473ff6896",
            "c12ce0cccbe14a749eda45d2302df8fd",
            "04835c4841d74a15bb7f33e8eb6b70bf",
            "0fd0c8c4324e489e8bf442c1d7f6930c",
            "dd363bfe0fb44118a7f4478d9f5500d6",
            "4ce078401be7494db7554377d2db5208",
            "5f9193a0db6c496196d8c4da2de89410",
            "39e509bd4a8e44e0be368a1379ca50ce",
            "c66d3c38792f4ecca3516b3c58e4edea",
            "befa33b77c7145bea00b073cc19ad2a8",
            "4cf8b2ab7fdd48f781cbdc3574d4299f",
            "aec55c7bb7fa4def9a627f55e070d3c0",
            "2ad1cad2fbff493abc4aa94d3eb0bded",
            "bcb57ff88ed349639b1377819bff934f"
          ]
        },
        "id": "v_3nQOF_bEc8",
        "outputId": "c3c7f246-8b7b-476b-fc0a-5ef19d1f8dbe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a987bfff0cf04819ad2394cd2ebf8d36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/936 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71207c2b00f444459ff5912bac0a1adc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': ' 이 이야기에서 얻을 수 있는 교훈으로 가장 적절한 것은?', 'context': '이제 한 편의 이야기를 들려 드립니다. 잘 듣고 물음에 답하십시오.\\n자, 여러분! 안녕하십니까? 오늘은 제가 어제 꾼 꿈 이야기 하날 들려 드리겠습니다. 전 꿈속에서 낯선 거리를 걷고 있었습니다. 그러다가 홍미로운 간판을 발견했답니다. 행 복을 파는 가게. 그렇게 쓰여 있었습니다. 전 호기심으로 문을 열고 들어갔답니다. 그곳 에서는 한 노인이 물건을 팔고 있었습니다. 전 잠시 머뭇거리다가 노인에게 다가가서 물 었습니다. 여기서는 무슨 물건을 파느냐고요. 노인은 미소를 지으며, 원하는 것은 뭐든 다 살 수 있다고 말했습니다. 저는 제 귀를 의심했습니다. \\'무엇이든 다?\\' 전 무엇을 사야 할까 생각하다가 말했답니다. \"사랑, 부귀 그리고 지혜하고 건강도 사고 싶습니다. 저 자신뿐 아니라 우리 가족 모두 를 위해서요. 지금 바로 살 수 있나요?\" 그러자 노인은 빙긋이 웃으며 대답했습니다. \"젊은이, 한번 잘 보게나. 여기에서 팔고 있는 것은 무르익은 과일이 아니라 씨앗이라 네. 앞으로 좋은 열매를 맺으려면 이 씨앗들을 잘 가꾸어야 할 걸세.\"', 'option#1': '새로운 세계에 대한 열망을 가져야 한다.', 'option#2': '주어진 기회를 능동적으로 활용해야 한다.', 'option#3': '큰 것을 얻으려면 작은 것은 버려야 한다.', 'option#4': '물질적 가치보다 정신적 가치를 중시해야 한다.', 'option#5': '소망하는 바를 성취하기 위해서는 노력을 해야 한다.', 'gold': 5, 'category': 'N/A', 'human_performance': 0.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/936 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fd0c8c4324e489e8bf442c1d7f6930c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"HAERAE-HUB/csatqa\", \"full\")\n",
        "print(dataset[\"test\"][0])\n",
        "\n",
        "ending_names = [\"option#1\", \"option#2\", \"option#3\", \"option#4\", \"option#5\"]\n",
        "\n",
        "def preprocess_function(examples):\n",
        "  first_sentences = [\n",
        "      [context] * 5 for context in examples[\"context\"]\n",
        "  ]\n",
        "  question_headers = examples[\"question\"]\n",
        "  second_sentences = [\n",
        "      [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n",
        "  ]\n",
        "  # 토큰화를 위해 1차원으로 평활화\n",
        "  first_sentences = sum(first_sentences, [])\n",
        "  second_sentences = sum(second_sentences, [])\n",
        "\n",
        "  # None 데이터 처리\n",
        "  first_sentences = [i if i else \"\" for i in first_sentences]\n",
        "  second_sentences = [i if i else \"\" for i in second_sentences]\n",
        "\n",
        "  tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
        "\n",
        "  # 토큰화 후 다시 2차원으로 재배열\n",
        "  result = {\n",
        "      k: [v[i:i+5] for i in range(0, len(v), 5)] for k, v in tokenized_examples.items()\n",
        "  }\n",
        "  result[\"labels\"] = [i-1 for i in examples[\"gold\"]]  # 원활한 collator 사용을 위한 변수명 이동, 레이블 0번부터 시작하게 변경\n",
        "\n",
        "  return result\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"test\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X_BAnhYlxl9",
        "outputId": "92145dbc-812e-44c6-aba9-8454a62d5647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
        "from typing import Optional, Union\n",
        "import torch\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForMultipleChoice:\n",
        "  tokenizer: PreTrainedTokenizerBase\n",
        "  padding: Union[bool, str, PaddingStrategy] = True\n",
        "  max_length: Optional[int] = None\n",
        "  pad_to_multiple_of: Optional[int] = None\n",
        "\n",
        "  def __call__(self, features):\n",
        "    label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
        "    labels = [feature.pop(label_name) for feature in features]\n",
        "\n",
        "    batch_size = len(features)\n",
        "    num_choices = len(features[0][\"input_ids\"])\n",
        "\n",
        "    flattened_features = [\n",
        "        [\n",
        "            {k: v[i] for k, v in feature.items()}\n",
        "            for i in range(num_choices)\n",
        "        ]\n",
        "        for feature in features\n",
        "    ]\n",
        "    flattened_features = sum(flattened_features, [])\n",
        "\n",
        "    batch = self.tokenizer.pad(\n",
        "        flattened_features,\n",
        "        padding=self.padding,\n",
        "        max_length=self.max_length,\n",
        "        pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
        "    batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
        "    return batch\n",
        "\n",
        "collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
        "batch = collator([tokenized_dataset[\"test\"][i] for i in range(5)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhWke8N7AYBj",
        "outputId": "49f80624-6598-4c42-99fc-184842acba31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2072,  0.1837,  0.1191,  0.2381,  0.2946],\n",
              "        [ 0.1826,  0.2586,  0.1497,  0.2627,  0.2450],\n",
              "        [ 0.0198, -0.0332,  0.0022, -0.0012,  0.0795],\n",
              "        [ 0.0885,  0.1150,  0.0509,  0.0649, -0.0134],\n",
              "        [ 0.3183,  0.3543,  0.3392,  0.3629,  0.3864]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  logits = model(**batch).logits\n",
        "\n",
        "logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj5XPsLsQ_E3"
      },
      "source": [
        "### 평가 지표"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvhpaMoZRIvy",
        "outputId": "48b67067-abd2-44f8-d99e-cdedf0ae8727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4 3 4 1 4]\n",
            "[4 4 0 3 1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'f1': 0.2}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "pred_labels = logits.argmax(dim=1).cpu().numpy()\n",
        "true_labels = batch[\"labels\"].numpy()\n",
        "print(pred_labels)\n",
        "print(true_labels)\n",
        "\n",
        "f1 = evaluate.load(\"f1\")\n",
        "f1.compute(predictions=pred_labels, references=true_labels, average=\"micro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_mwvhlQhfz-"
      },
      "source": [
        "# 5.2.4 Token Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z5sff86i1Q5"
      },
      "source": [
        "### 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VssOBtTLRg_4",
        "outputId": "5f570fb6-0eb7-4525-9387-03f382c52f0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "model_name = \"klue/bert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7OVYAoCCybQ"
      },
      "source": [
        "### 데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201,
          "referenced_widgets": [
            "73b7b0b316d444a49d2c882357b3bf7d",
            "80edb25b766a47a8a9257c622d2dbfc9",
            "3b127b3de2fa4719953621bf935f141e",
            "fea862dc74554ab3bbe5c76acfa893ca",
            "48c69fc498ce4bbaad0caed78bf2fa52",
            "0903ea6e68ae43d99cf514536bcef9bf",
            "1053f9b55fb44a1797de75e9853a5dce",
            "e5899eb8c54a4762a277dd9d2f814267",
            "e2814554335942e8a6d825f8ab61ac9b",
            "7e1e55cbd63b4025ae99f167da86744c",
            "dd5a072b7587466c80b223bef3e8ee76",
            "170034357031458692bf56d97b1b4915",
            "50d28a4f6e7148ccb71c90b57a7bcdda",
            "dac90c0538bb4af8b9d002cdb01cc4e1",
            "2c7785a0aae54602b7a808b186185781",
            "68ccc65db40e43a59a482063f3f55e59",
            "63aebbd0889e4292baa2fba09fc996c7",
            "a5abd835017146c8b71cca1ad985d7c0",
            "42ab724d373b4d54926034ac029652b6",
            "8ee560f57ddf434d8307b76bd3913a6f",
            "745cd7afacf9493199c6acde2eac2f8c",
            "46440e46303e407a89e4044169005d57",
            "61903ff1cf9b4da98dbdc41951e84b45",
            "62a6f3ac1b094494ad7b6cdace87d159",
            "0a64310ecc5d48c3ae24a047a45582b5",
            "186cba0e2a414ef68014922ea2e8a863",
            "1b32c9a5cec8422ea77140e32772de3c",
            "8d50a9407dd3414ba1542578ad5d5e5b",
            "9fb4ea2e00444699a708f87576d1737c",
            "b91257f163d7437d9a0cc1fa0d9a2e42",
            "96352a09b1f942cc935517c4cc8f792f",
            "a93a79feb2d448d89c9612b68e1be51e",
            "3a1e6d3405d642b5ab1b79ad47c47600",
            "3379cb74ee6b4908b709f9261c2926f8",
            "1da6314b0bac4e3b85d354fea14ceac7",
            "c5a2a6c9233047129b869bf62a441b21",
            "a4ed243f5cea4a26b93192529410ac1c",
            "eba6773a4db0451682e099cf2787c019",
            "bcc3ce14d87542359c1c1307269a9893",
            "057efd364aad41ca9f0819edd7f4a340",
            "23673096622b47a1944b1500f0e707cf",
            "f2d3b5531b4f4564b2ec2486e6f24443",
            "e7ee608a4eee4f73ab4e41e8491d1b1e",
            "1fbfab231e23459f9ede99903b33682d"
          ]
        },
        "id": "0rEcW7_2Flqo",
        "outputId": "91f814ce-7679-4ef7-f286-f326da58ab7e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/4.21M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73b7b0b316d444a49d2c882357b3bf7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "170034357031458692bf56d97b1b4915"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/21008 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61903ff1cf9b4da98dbdc41951e84b45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3379cb74ee6b4908b709f9261c2926f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens :  ['특', '히', ' ', '영', '동', '고', '속', '도', '로', ' ', '강', '릉', ' ', '방', '향', ' ', '문', '막', '휴', '게']\n",
            "ner tags :  [12, 12, 12, 2, 3, 3, 3, 3, 3, 12, 2, 3, 12, 12, 12, 12, 2, 3, 3, 3]\n",
            "(66, 66)\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"klue\", \"ner\")\n",
        "\n",
        "sample = dataset[\"train\"][0]\n",
        "print(\"tokens : \", sample[\"tokens\"][: 20])\n",
        "print(\"ner tags : \", sample[\"ner_tags\"][: 20])\n",
        "print((len(sample[\"tokens\"]), len(sample[\"tokens\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54IWM29_tYH9",
        "outputId": "09f3d57b-0793-43b4-8f45-e1642519eb8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "특 \t 12\n",
            "히 \t 12\n",
            "  \t 12\n",
            "영 \t 2\n",
            "동 \t 3\n",
            "고 \t 3\n",
            "속 \t 3\n",
            "도 \t 3\n",
            "로 \t 3\n",
            "  \t 12\n",
            "강 \t 2\n",
            "릉 \t 3\n",
            "  \t 12\n",
            "방 \t 12\n",
            "향 \t 12\n",
            "  \t 12\n",
            "문 \t 2\n",
            "막 \t 3\n",
            "휴 \t 3\n",
            "게 \t 3\n",
            "소 \t 3\n",
            "에 \t 12\n",
            "서 \t 12\n",
            "  \t 12\n",
            "만 \t 2\n",
            "종 \t 3\n",
            "분 \t 3\n",
            "기 \t 3\n",
            "점 \t 3\n",
            "까 \t 12\n",
            "지 \t 12\n",
            "  \t 12\n",
            "5 \t 8\n",
            "㎞ \t 9\n",
            "  \t 12\n",
            "구 \t 12\n",
            "간 \t 12\n",
            "에 \t 12\n",
            "는 \t 12\n",
            "  \t 12\n",
            "승 \t 12\n",
            "용 \t 12\n",
            "차 \t 12\n",
            "  \t 12\n",
            "전 \t 12\n",
            "용 \t 12\n",
            "  \t 12\n",
            "임 \t 12\n",
            "시 \t 12\n",
            "  \t 12\n",
            "갓 \t 12\n",
            "길 \t 12\n",
            "차 \t 12\n",
            "로 \t 12\n",
            "제 \t 12\n",
            "를 \t 12\n",
            "  \t 12\n",
            "운 \t 12\n",
            "영 \t 12\n",
            "하 \t 12\n",
            "기 \t 12\n",
            "로 \t 12\n",
            "  \t 12\n",
            "했 \t 12\n",
            "다 \t 12\n",
            ". \t 12\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(sample[\"ner_tags\"])):\n",
        "  print(sample[\"tokens\"][i], \"\\t\", sample[\"ner_tags\"][i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "c0f3f2ece54e4e82ab5a6be810a0b08d",
            "ac5158df8dfc4b6a9d0825953b6654dd",
            "20d61565f777470cbc642a59a35dae29",
            "796f7aaf072642169ebc515941a87a4a",
            "1d8a7ae90c2446aaaf0c65c32d9c1cdc",
            "5b4de8b8280d4283b218be2e1b570aa7",
            "e0378576e03d4171a4347afbf837100f",
            "2e5f9590c7b9420fbed099a005baabb6",
            "72f788e35105402e96608675a3176a16",
            "bc507135ddea4c83922dc2f06cfd2312",
            "4a9c4350cd37453abaa4dbb956294f2f",
            "71b09b11b1aa44d1874ff9d4b23d897e",
            "a86de964744447b3a4c3c2a384ce2f43",
            "85625bc533ff472cbe27f5e898777bda",
            "72dfc7ff905d4f8abf138a6993a1dd2f",
            "06ddf6d71c4a4f93ba88a06db0e6df37",
            "99d0ff86f775473588de0461caea3fa9",
            "db0e6340418945f5af7f71fa88abcb62",
            "c2e2e1ec280249cab8b91230673a917e",
            "a27d9b590e8a44c481ee4a801b3cf808",
            "a86eba45255f46e3acba3f5424d62e06",
            "fca3296a52ac42cfb1acd4e7a19f5120"
          ]
        },
        "id": "p3InmXddjfKv",
        "outputId": "d25ffa88-fe4c-48ed-922a-7e00e34bd4ac"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/21008 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0f3f2ece54e4e82ab5a6be810a0b08d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71b09b11b1aa44d1874ff9d4b23d897e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # 토큰을 해당 단어에 매핑\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:  # 스페셜 토큰을 -100으로 세팅\n",
        "            if word_idx is None:\n",
        "                label_ids.append(12)\n",
        "                # label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:  # 주어진 단어의 첫 번째 토큰에만 레이블을 지정\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True, remove_columns=dataset[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cerYk3zvAqKC"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "batch = data_collator([tokenized_dataset[\"train\"][i] for i in range(10)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "z9P2HxeeUSDp"
      },
      "outputs": [],
      "source": [
        "id2label = {\n",
        "    0: \"B-DT\",\n",
        "    1: \"I-DT\",\n",
        "    2: \"B-LC\",\n",
        "    3: \"I-LC\",\n",
        "    4: \"B-OG\",\n",
        "    5: \"I-OG\",\n",
        "    6: \"B-PS\",\n",
        "    7: \"I-PS\",\n",
        "    8: \"B-QT\",\n",
        "    9: \"I-QT\",\n",
        "    10: \"B-TI\",\n",
        "    11: \"I-TI\",\n",
        "    12: \"O\",\n",
        "}\n",
        "label2id = {\n",
        "    \"B-DT\": 0,\n",
        "    \"I-DT\": 1,\n",
        "    \"B-LC\": 2,\n",
        "    \"I-LC\": 3,\n",
        "    \"B-OG\": 4,\n",
        "    \"I-OG\": 5,\n",
        "    \"B-PS\": 6,\n",
        "    \"I-PS\": 7,\n",
        "    \"B-QT\": 8,\n",
        "    \"I-QT\": 9,\n",
        "    \"B-TI\": 10,\n",
        "    \"I-TI\": 11,\n",
        "    \"O\": 12,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QYEk-vVArUa",
        "outputId": "ce2d99b2-0506-4edd-eba6-f61b498a02e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"klue/bert-base\", num_labels=13, id2label=id2label, label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBOmWNMa40e6",
        "outputId": "7b184b85-d594-42ad-a123-bae64e2b4c4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B-PS',\n",
              " 'O',\n",
              " 'B-TI',\n",
              " 'B-OG',\n",
              " 'I-DT',\n",
              " 'I-QT',\n",
              " 'O',\n",
              " 'B-PS',\n",
              " 'B-TI',\n",
              " 'O',\n",
              " 'I-QT',\n",
              " 'I-LC',\n",
              " 'B-TI',\n",
              " 'O',\n",
              " 'I-QT',\n",
              " 'I-TI',\n",
              " 'B-QT',\n",
              " 'I-DT',\n",
              " 'B-QT',\n",
              " 'B-QT',\n",
              " 'I-DT',\n",
              " 'O',\n",
              " 'I-TI',\n",
              " 'I-DT',\n",
              " 'O',\n",
              " 'I-TI',\n",
              " 'B-PS',\n",
              " 'I-QT',\n",
              " 'B-TI',\n",
              " 'B-PS',\n",
              " 'B-PS',\n",
              " 'B-TI',\n",
              " 'B-QT',\n",
              " 'B-QT',\n",
              " 'I-QT',\n",
              " 'B-DT',\n",
              " 'I-TI',\n",
              " 'B-QT',\n",
              " 'O',\n",
              " 'B-TI',\n",
              " 'B-TI',\n",
              " 'B-PS',\n",
              " 'B-PS',\n",
              " 'I-QT',\n",
              " 'B-PS',\n",
              " 'B-PS',\n",
              " 'I-QT',\n",
              " 'B-DT',\n",
              " 'B-QT',\n",
              " 'I-DT',\n",
              " 'B-TI',\n",
              " 'I-PS',\n",
              " 'I-DT',\n",
              " 'O',\n",
              " 'O',\n",
              " 'B-PS',\n",
              " 'B-PS',\n",
              " 'B-TI',\n",
              " 'B-TI',\n",
              " 'I-TI',\n",
              " 'O',\n",
              " 'I-QT',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'I-QT',\n",
              " 'I-QT',\n",
              " 'I-QT',\n",
              " 'B-PS',\n",
              " 'I-QT',\n",
              " 'I-TI',\n",
              " 'I-QT',\n",
              " 'I-QT',\n",
              " 'B-PS',\n",
              " 'B-PS',\n",
              " 'O',\n",
              " 'O',\n",
              " 'B-TI',\n",
              " 'B-PS',\n",
              " 'O',\n",
              " 'O',\n",
              " 'B-TI',\n",
              " 'B-TI',\n",
              " 'I-OG',\n",
              " 'B-TI',\n",
              " 'B-TI',\n",
              " 'B-TI',\n",
              " 'B-PS',\n",
              " 'B-TI',\n",
              " 'B-TI',\n",
              " 'I-QT',\n",
              " 'I-QT',\n",
              " 'I-QT',\n",
              " 'I-QT',\n",
              " 'B-TI',\n",
              " 'B-TI',\n",
              " 'I-QT',\n",
              " 'I-QT',\n",
              " 'I-QT',\n",
              " 'I-QT',\n",
              " 'I-QT',\n",
              " 'B-TI',\n",
              " 'O',\n",
              " 'O',\n",
              " 'B-TI',\n",
              " 'B-TI',\n",
              " 'B-PS',\n",
              " 'B-QT',\n",
              " 'I-QT',\n",
              " 'O',\n",
              " 'O',\n",
              " 'I-QT',\n",
              " 'I-TI',\n",
              " 'B-TI',\n",
              " 'I-QT',\n",
              " 'B-TI',\n",
              " 'I-QT']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  logits = model(**batch).logits\n",
        "\n",
        "predictions = torch.argmax(logits, dim=2)\n",
        "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
        "predicted_token_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zra3gwa-EBk5"
      },
      "source": [
        "### 평가 지표"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hT5bm8TO5Ues",
        "outputId": "0ebb657c-9fcf-4f52-f926-056286ac4b6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'f1': 0.023076923076923078}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "pred_labels = logits.argmax(dim=-1).view(-1).cpu().numpy()\n",
        "true_labels = batch[\"labels\"].view(-1).numpy()\n",
        "pred_labels.shape, true_labels.shape\n",
        "\n",
        "f1 = evaluate.load(\"f1\")\n",
        "f1.compute(predictions=pred_labels, references=true_labels, average=\"micro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US1Ha7dVE-B7"
      },
      "source": [
        "# 5.2.5 Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxl3QORm6GX-",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXMEP_A8Ef6L",
        "outputId": "d15b502e-168a-4015-baed-ce33c734a9ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForQuestionAnswering(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "model_name = \"klue/bert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TTrJBaC6FUR"
      },
      "source": [
        "### 데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201,
          "referenced_widgets": [
            "2b7168a2dc50487c836677bcd875be16",
            "e0d329b8be2a4206b78ed662f990f47f",
            "72537714e3d945ffb6c7114b628ea692",
            "7a8b3930215e4b8b8a6df20886ae0492",
            "2cba14e8e282496482766c35f19391d5",
            "3b1fd0fa503f4fa69818676eac49110c",
            "e04265a355474556b555c3c187b9181a",
            "2acf9c69dc034cfabd8e92b43284f352",
            "2c165f77e6af47fca55aed38770e95cb",
            "8004ccf62fed49f9996977d6037c6225",
            "0e56ed518ac64ec7bb4ddc7048d693f4",
            "bfa305a53c944d41b8d492488581c6f7",
            "91cb8c805dba4aeb9dccd13d27463cc5",
            "1f0aceaa21ce4321af8144fe968216cc",
            "9124e5f9f34049e38488dc465099c64a",
            "218ea4f4017548cf95941c681e7fd9f2",
            "35eb6bf80d16411d8afc993d0164709d",
            "ca84b1d191424ab5a623122ac27dad76",
            "e80f10bd9e2c4cd084a5920e533ba1f1",
            "d47c54743fc54c818ac519ba62b13193",
            "9a88a5323b4643468f863c44c2ded3dc",
            "79f8b5be28a44bcb8787d3a30b29f65d",
            "1fa6a3c4046a4b74967d1d4d495fad27",
            "d0cf69f6bd50442184ebe1da7cb9e5c4",
            "1b18d1e73bfa4fdf8d43155ba4cfadbe",
            "fa6a6f7d32a54007b0a0707e6efa22ca",
            "a55fa8f5abc842aeb8e0f757843998db",
            "199c3ad043084123b8820271b2803a5b",
            "1a99bbf048404de4b4a39cd58e786d54",
            "8f2259cc35ff4738a0ed5226092ffe88",
            "0784f90a25ac4042b17ab0969d83f6cd",
            "37007ccc0438428eb6051e93fe24127e",
            "7f9b92732ef1435c82176e45f3723992",
            "9ed7fb02905a45a4b99532de72c2671c",
            "456ccf036f7e463f8b6a42e3ea0d14b1",
            "6be0b69e82774ad5b9fe50b494ba17bc",
            "4ab19ee29c1040c8aaa93407e9f1849c",
            "2d30a4b5f9af4860b03fafa34f8eadfb",
            "1b9a2e78217d4170b0403b927d2f84c3",
            "4ad1a50e08be4ea38b3cf01414b32482",
            "91afe5f2d91c4f8a87ceefc37febed2d",
            "9b439bdee3574167a14727a6114814df",
            "ffeb9898b3434428b783d85de19174cf",
            "66c14391d868402fbe3c56afc435e933"
          ]
        },
        "id": "yY_qte4E6812",
        "outputId": "fc438338-e459-4a87-8aff-64b881e07169"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b7168a2dc50487c836677bcd875be16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/8.68M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfa305a53c944d41b8d492488581c6f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/17554 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fa6a3c4046a4b74967d1d4d495fad27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/5841 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ed7fb02905a45a4b99532de72c2671c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "내용 : 올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 \n",
            "질문 : 북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?\n",
            "답변 : {'answer_start': [478, 478], 'text': ['한 달가량', '한 달']}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"klue\", \"mrc\")\n",
        "sample = dataset[\"train\"][0]\n",
        "\n",
        "print(f\"내용 : {sample['context'][:50]}\")\n",
        "print(f\"질문 : {sample['question']}\")\n",
        "print(f\"답변 : {sample['answers']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "d361ffc05bfd4f228ccf9fa4b9f0300d",
            "d8d71cc2c41b42a1a0746b760bab3dff",
            "5d867223d3524dd687e2cb5c80a3c95e",
            "cdaa04ca94f140d6bb076a2df9de734e",
            "e3f99679a1904c319048df1f33ce2f2e",
            "8d1d81d7332a4b51a13ec164a4fb70e1",
            "a922513b84c942f4ad1f06ab01b890be",
            "e6af9539813e467489fb9e21b86a9fb1",
            "bade5cfd45694ef7ba639f8e3fded67a",
            "364ace4b22774a559a039ce7d549347b",
            "9561b492489d485388388321bd3fee91",
            "328712b033ef4be2b6b4c15372cb61f4",
            "3a1c2b280fac45a595ee14bcc50f4fdf",
            "832e657cb2b9472e95b313acd1fd5329",
            "bf20f38323704c908080893264d128f1",
            "1e951979b072422faebc13a4f552fff9",
            "1b53c8d3912d4c16930e1a317319393e",
            "47ffe3b74bc14c2e8585ebc88d9bfbab",
            "850577fcc4df4b238e26898128a2c308",
            "21daf2556d4649e8ada4af71044d622c",
            "866adb270a754a3fb25639a2d5a03692",
            "b6e41990037c460ab99d313dee1728e9"
          ]
        },
        "id": "XhL5OJ_b7In0",
        "outputId": "370b5cdc-5b0c-4f2a-f635-69006944b62e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/17554 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d361ffc05bfd4f228ccf9fa4b9f0300d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5841 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "328712b033ef4be2b6b4c15372cb61f4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4ZDrKfRDaDD",
        "outputId": "54ded5cc-abfc-4572-bb60-2e7f9ebcd2f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[    2,  1174, 18956,  ...,  2170,  2259,     3],\n",
              "         [    2,  3920, 31221,  ...,  8055,  2867,     3],\n",
              "         [    2,  8813,  2444,  ...,  3691,  4538,     3],\n",
              "         ...,\n",
              "         [    2,  6860, 19364,  ...,  2532,  6370,     3],\n",
              "         [    2, 27463, 23413,  ..., 21786,  2069,     3],\n",
              "         [    2,  3659,  2170,  ...,  2470,  3703,     3]]),\n",
              " 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
              "         [0, 0, 0,  ..., 1, 1, 1],\n",
              "         [0, 0, 0,  ..., 1, 1, 1],\n",
              "         ...,\n",
              "         [0, 0, 0,  ..., 1, 1, 1],\n",
              "         [0, 0, 0,  ..., 1, 1, 1],\n",
              "         [0, 0, 0,  ..., 1, 1, 1]]),\n",
              " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
              " 'start_positions': tensor([260,  31,   0,  80,  72,  81, 216, 348, 323, 348]),\n",
              " 'end_positions': tensor([263,  33,   0,  81,  78,  87, 221, 352, 328, 353])}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()\n",
        "batch = data_collator([tokenized_dataset[\"train\"][i] for i in range(10)])\n",
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "7YbodU1KKU6s",
        "outputId": "07a93aa7-6be2-44a7-8570-df0736bdac68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'##한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다. 장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20 ~ 21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24 ~ 25일이었으며 장마기간은 32일, 강수일수는 17. 2일이었다. 기상청은 올해 장마기간의 평균 강수량이 350 ~ 400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(**batch)\n",
        "\n",
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()\n",
        "\n",
        "predict_answer_tokens = batch[\"input_ids\"][0, answer_start_index : answer_end_index + 1]\n",
        "tokenizer.decode(predict_answer_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "hCP2LIt-El52"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
   
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
