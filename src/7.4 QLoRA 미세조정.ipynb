{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Uj6Ntl5U7S-4",
      "metadata": {
        "id": "Uj6Ntl5U7S-4",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ccc6284-a381-47d8-93ef-51edace63cfc",
      "metadata": {
        "id": "0ccc6284-a381-47d8-93ef-51edace63cfc"
      },
      "outputs": [],
      "source": [
        "!pip install \\\n",
        "     scikit-learn==1.4.2 \\\n",
        "     transformers==4.43.0 \\\n",
        "     datasets==2.20.0 \\\n",
        "     peft==0.10.0 \\\n",
        "     accelerate==0.32.1 \\\n",
        "     bitsandbytes==0.43.1 \\\n",
        "     trl==0.9.6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe4988ed-b378-4c7a-a664-d2514c1f2523",
      "metadata": {
        "id": "fe4988ed-b378-4c7a-a664-d2514c1f2523"
      },
      "source": [
        "# 7.4 QLoRA 미세조정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wK0mLOCn8HF1",
      "metadata": {
        "id": "wK0mLOCn8HF1"
      },
      "source": [
        "## 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ghuQ7jy0ANmp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536,
          "referenced_widgets": [
            "efe36e33c6804c9fb32fcc1bc2046d63",
            "e80158302f3c4b518cd097e9e1df44d4",
            "55d764080c3f448d9d909d85ffcdeee7",
            "88669a1e3f7845a3a7815d5043819e97",
            "8cf7a90c1b96439eb3c26208311621dd",
            "effe9fa5fb9f4d9b9a74e63372efdc18",
            "92ef675b8ba04b20ba8a324b253d17a8",
            "55a4bc9453a64d57a76e40d4c1b1a616",
            "0122835ce9a3444bb22419a69547ca88",
            "f3c6ddb96c114aa4b6eb202e21071cd3",
            "1c3ea68b0b7344ddb135fb381b2f4ea0",
            "aac4f32283cd4d27b3490434da447cb9",
            "1e4c393e186743d6a5741902582b3a6b",
            "f0e332b2682a47299ea05c200064d7bf",
            "47fb83073d1c492da2602456e1cb1f78",
            "36a535bf2fab4e43930c287a27ac11b5",
            "8b15973dd3144caf8693d1ef21845fec",
            "6a0d20262ab347ed8be29e90ad6c63ce",
            "55b9c911a04c4d5f9e59fcaba2f00db5",
            "9368e4b033814cafa07d25de9da782a4",
            "bdd5c6ff7a624ee2be04bc43e2a9838f",
            "236ba490d73341a9bda4c63a6ca3d379",
            "85c0a7da2e324e72983ff4e111f74096",
            "4dd9987bfdc244dcb41a99d153c252a8",
            "cf22c0cf73b74e67b3f2a2bbef97fbbe",
            "5f02de0d6f1e41d188bce44f540876b0",
            "8b3a2af7fdf847f08162cb63a60dc26c",
            "7f84706019384b899c28fc5570f8df55",
            "6aa59bda085f44ce822d7a6a0a6ab82b",
            "e3e67b77d85343dc8f558c65a14e81e8",
            "a158a1d4087e4ca6955fc46925d4f6a7",
            "8dfea1cc757a40a99ab169ce86be4d42",
            "173baa84283c4641b9d2297449891623",
            "ac22bca6934840d78ee2ad40795ffbb7",
            "58c0adb6b3f74751975c879c809a927a",
            "035f30a3b0294ea29906e2bf7c44208d",
            "1e0036a0153e4615b94a952c7e187f86",
            "f5809fa553464b0a95ea917de086f327",
            "ecacd9d3c9c04548b8774ff91bcf07fa",
            "a8094bfb1eb4472abb6cdfc871015554",
            "e77e10f9e46a4a5b9ce975bc65abe871",
            "3fba033e7f514c0092c5373990ff3a09",
            "a79b6f6ed9c941e4868d3a5ecc51d760",
            "0ef2152e7d1b4389916b50ecaa758c3c",
            "370afb16d4e04d9fab01686633f744d8",
            "6f4b79a0b3a046709cb11f726ea62136",
            "f8960ff207274a5d8811bb7ec458a41c",
            "93837819d7be4e03b40827aa2d94f3f4",
            "ce39cc4a078146c1849f776d0a0091b9",
            "94b9b05f06414b8b845980bbe8e5bcbc",
            "5f07eb4b5ca046629204fc38e5715a45",
            "b1ca2b8f63fb4bcf90d0a18c06365a41",
            "a13babb43a4449cc9db6423dc9f3a98f",
            "2d30f772e4cc4301937f4353a8f07ea8",
            "2aa14292b29e4df490ebbf8fb0d30690",
            "f67e1b883a3041f1aef301086e2b21f2",
            "f69b9a3bd359418fbfec53f9e95dd6da",
            "ea167b7c415047388535abf9cbe31979",
            "fb34db0832d449ae8e3fd6391222f152",
            "4c3706e20c9244d1a0e5a8b6f20fae05",
            "52297bd97ba74f0d86b8cbea10d096d1",
            "d69a7f31fe3447f093aef1089da64dc7",
            "d1417173ea644f1f9ee7ba627ca1dc7a",
            "6c18352b13254994b8144d7cac9d2a59",
            "8d64088da6974fb58e975a6f5a13f86c",
            "0d604b557c8e4dbfb6d0fabf8e5d06cd",
            "3aae394a581f45f08a93db9c66e092fb",
            "d1b0881fda6a4e6bb6ff3cf7e5a1b2b1",
            "405e9e316b6b4dde8dae63c68f10e0e4",
            "fa0911b6e40f4eb4a7a7c89b08de45fe",
            "f6a15990687c4dc69f963bc986259534",
            "e70c3ac3185f4688bcb4db81f7d7012e",
            "d74675d6bb394e5ebed0dec41cf8429a",
            "0b54c8f313114b1a8505538726b8e61d",
            "b05648a2d21945599405aeab53e3dce4",
            "10c8c9a62cf5436a89c84e90fd7ef50a",
            "06b12142381a4dd8ad59261a4f39c574",
            "8e23ff557cfd47b3846dae27d903ef3f",
            "427e1c1ab4df44df8b36a092ded73e22",
            "e64171d252594533bedafba0414644e9",
            "f28b4740f3e94184a9854c4466f1f2fe",
            "e4742cb5b14b490a9df649ff4e682396",
            "15d870d418d447708d1f541f44e7d099",
            "9a977a74b5a54903ba568e90c4a5fb37",
            "71807427d85e4db1a89fd4f5d5888bb2",
            "c99e8093cd27463481e1d1d259e2400d",
            "db3e7c85b6db470eb0a75b3e0624add4",
            "56e45429a1064a46acb9152b14593f95",
            "f06d5f56ee224604bb006dbecff9677e",
            "9a93d392618c4c5a8999120840778307",
            "b452f1235e3744cd91be52930a9539b0",
            "2ea72535368d4934a236bd50e446d399",
            "cfbc549639e6439c99e514ce26053a4b",
            "8142753abbcf4ac8998446983fcbe87a",
            "32ff7c4aedac4e9fb3650e0d81e18d12",
            "077b4a0681ae4e629c4e0e624fbbcea0",
            "4047b256d85d4b58955e7303723762c1",
            "aca7d02d67254239853fcb017a138541",
            "d3313e3b33bd44788a5813039acc8585",
            "84f2ed2cdd1c4a32a8d019f837bfb1a4",
            "508b14f9307643a2bdcf6af7a6871793",
            "f780350120cb4329b769e7d09a9075b6",
            "35deccb9d6ce419da48e6106951cd0ca",
            "c3c65a1ecc6a4a1f928294021ef4ced1",
            "d73580cd1fcb4c9488aa428767d828fe",
            "64808ea591f6417b98114850698d9643",
            "013f95e8f805421aa3258c46d26f46b0",
            "955ff268bc6f42b3a5f321cd3f740a94",
            "8f71241f41d4467b92aa334f481cd2e2",
            "de418c16acf04fa69ccf542ebcf814b6",
            "9215c7ab4c1a45f287576aab595485c8"
          ]
        },
        "id": "ghuQ7jy0ANmp",
        "outputId": "be38bac6-6010-4486-ca08-526d8be47c92"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9215c7ab4c1a45f287576aab595485c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    GenerationConfig,\n",
        ")\n",
        "\n",
        "model_name = \"nlpai-lab/ko-gemma-2b-v1\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"float16\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "gen_cfg = GenerationConfig(\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.92,\n",
        "    return_full_text=False,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77eaa7dd-2385-4e27-9ad9-b9753bb2b1e0",
      "metadata": {
        "id": "77eaa7dd-2385-4e27-9ad9-b9753bb2b1e0"
      },
      "source": [
        "## 데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93a4f224-dc21-4ca5-b732-ea22beceb70c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445,
          "referenced_widgets": [
            "4ef61ae5727344909d6e1c1872bec5ab",
            "fe21d5e1cc8c40b0b755bdb95ebef655",
            "ef36df9a24394fc8bb41bb199dc72bbc",
            "bf0e754a71c3433e94cd605369f5b302",
            "55a93d94dd2a418a871cbf5b43691591",
            "7bda696d20924b999522ae029a263a38",
            "40824063eb524a5a8fced46f4c9b3f20",
            "9b3061fc06c34c88bf4052be71e4f0c3",
            "837cd01fd7f647cdb98bfd629f57e982",
            "9d08e4ed2b124cd997ed1f3dbd292fb9",
            "551e841e5869488982ce245143be6095",
            "08218a82616c4750800ede167ed7f3e3",
            "85453db3d1f84db9a22e2eca3077f1c0",
            "f3f4a276ce88421c8fc76de5d414e922",
            "a31cd676da6a4cc297ddbb49ead62448",
            "872987e3297f45aea2f3e59c5017c82b",
            "1e6e695ede3c471e9508d50090b593ff",
            "52c8a7f57c4b4c12b86a9bf00a6502d1",
            "dfc051b7711b4979b114a70bccbab354",
            "aa8b8362e8bc42fa86f1f061dfe413c4",
            "beed84e136c94abe9ca5c3bd8d4bc380",
            "2499a5f0e0dd41349e1511b4c2db5fa3",
            "d23c820b4df5494fb5ad5f43f4261507",
            "0872e98150b343df90b47f97b341b123",
            "939051778d574bb485dd5666eed16d2e",
            "e180af11b63b4f8cbad9de13db470a9d",
            "eba7d1f212304ff69411c9bfe99f9e72",
            "ea670bff28ba49f9b955f88cf01f2307",
            "af50cbb1e0044753b345676eb2414a8d",
            "193af6e9a98242488a63cd83540e993d",
            "29062c732a1e4154a0f5454b0d364c94",
            "0809f92751244908bee42da250bca306",
            "584c1a6299604641873af50a92aa102d",
            "2b8d08cb021344b5aa00640f15f5464a",
            "cad3036fe88641f287c34b8a80cba43a",
            "191f6575f2c2455582d634fce667daa8",
            "ac00dad424584ba09901699c1202d080",
            "e5754abf3a224b26b317ffdb17ec2c84",
            "0ad2bb87babd4741989bdae68b84b787",
            "172c1ed7f034488185700f5a836bbeaf",
            "126cdcecf82a40b48deb1b1fb9fa2af3",
            "bf3a22cf6da64d0aa89b93847d3f42e0",
            "d4c1ef1fc73c496a87019beb977ab616",
            "d2c5da39bd5c4e08a210b906262e3ada",
            "c8f84a06cfdc4ebea5919514e8c3dbf1",
            "d2546f7da1334936a1dfb348457ea493",
            "deec2a4372dc498d88d15821733d9686",
            "66b99203e258469ba589a630d6abe103",
            "fddd87a3034346bca471372bdfcbfdd4",
            "9c99cc40eaab4a47ba12d856cd0e370e",
            "aa93ac8241a94c53afce5144ccda28e9",
            "133db58e7e694b24bc92f5aa0c8877a8",
            "89d455de64014622bdfa1f4bdcb2cc55",
            "dc05673642364a8ebc84e1ac0bd7017d",
            "7d808cadcd0f487e805b6c2aef763481"
          ]
        },
        "id": "93a4f224-dc21-4ca5-b732-ea22beceb70c",
        "outputId": "52dbded6-484c-4fc2-dda5-d99a8a375a89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'title': '제주도 장마 시작 … 중부는 이달 말부터',\n",
              " 'context': '올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.',\n",
              " 'news_category': '종합',\n",
              " 'source': 'hankyung',\n",
              " 'guid': 'klue-mrc-v1_train_12759',\n",
              " 'is_impossible': False,\n",
              " 'question_type': 1,\n",
              " 'question': '북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?',\n",
              " 'answers': {'answer_start': [478, 478], 'text': ['한 달가량', '한 달']}}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"klue\", \"mrc\")\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5a4d643-712e-47a0-9cc8-655cafc174e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5a4d643-712e-47a0-9cc8-655cafc174e8",
        "outputId": "b9c47de1-eb37-4588-ed10-d8751108193a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bos><start_of_turn>user\n",
            "다음 질문에 대해 대답해 주세요.\n",
            "질문 : 북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "북태평양 기단과 오호츠크 해양 기단이 만나면서 전 세계에 바로 가는 지점은 아무 곳도 없습니다. 북태평양 기단은 북극을 향해 이동하고 있으며, 오호츠크 해양 기단은 아시아 서부 지역을 따라 이동하고 있습니다. 이러한 기단들이 국내에 머무르는 기간은 주로 이동의 방향에 따라 달라집니다. 북태평양 기단은 북극으로 향하고 오호츠크 해양 기단은 아시아 서부로 향합니다. 북태평양 기단이 북극으로 향한 경우, 미국 북서부 지역에 머무르며, 오호츠크 해양 기단이 아시아 서부 지역으로 향한 경우, 남동쪽 미국 중부 지역에 머무르게 됩니다. 이러한 방향에 따라 지속된 기간은 다양할 수 있습니다.<eos>\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "doc = dataset[\"train\"][\"question\"][0]\n",
        "messages = [{\"role\": \"user\", \"content\": f\"다음 질문에 대해 대답해 주세요.\\n질문 : {doc}\"}]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs.to(model.device),\n",
        "        generation_config=gen_cfg,\n",
        "    )\n",
        "    print(tokenizer.decode(outputs[0].cpu().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zrWkPIta8p_4",
      "metadata": {
        "id": "zrWkPIta8p_4"
      },
      "source": [
        "## QLoRA 미세조정 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m65xNb129ply",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "52b56db396f448bbb99e894d6047ab82",
            "640846fc936d400e96064303c6195ade",
            "a14087f3ffb14136aa3af7e140a7aa18",
            "95bb540277584ebe819b289f8ef0b2fa",
            "3f36169edfd2412ea4e2d53c01b7a6c5",
            "40db6b2d2e0d498d97f56baef0f3b269",
            "ea93707b81654b0e8ec6085082c8b3c7",
            "a25e30c8e26548a08315f74af3d113a5",
            "37bf93db44e3429eb94293b8d1a4ef43",
            "d40acb9ae6d0483ba86b3e7934ca5828",
            "12ffd962deb34fa9a537cbac33785b4a",
            "0f994786f19b418f91253559095fe0b0"
          ]
        },
        "id": "m65xNb129ply",
        "outputId": "e7dacdf3-00dd-4505-cbb4-f818e3d1b476"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f994786f19b418f91253559095fe0b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nlpai-lab/ko-gemma-2b-v1\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"nlpai-lab/ko-gemma-2b-v1\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rVONk_0-XWba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "fd721dfc64f744929b2834297c9e82c4",
            "0ad07b491eb24cbc8fd0f274ed4cee11",
            "f7a0906740494004aaaeb496c6d54f46",
            "8190b103df0347d0ae05e569c7e7cd92",
            "5946b144627d4a16ba86c526663eedcd",
            "2f05def728a14e10a55197fdb786b277",
            "c3840a1e3de347ed8443958ed3795281",
            "5b5ef9417c674e5e816bdc38dc9afe99",
            "af68b0d5b5dc49e5a146167a79a216c9",
            "c6e941e190e6490aae6378a167f01bab",
            "9a522025f0cf40dabcf871ab42d6e9c3",
            "7a71c44c98b2498a9336608feb2825a5",
            "e3e59f87e1f9404db23155636dabc533",
            "a1a32d5c47fa44f69476ae2a1ad2c736",
            "e4797b0260754bd1a8580c820e83c558",
            "cc93449111d743deab3764e4f3f9fabf",
            "56490119d5a741648cba27ca59c060b1",
            "08eb859f399348ba9db6b477285bdb9e",
            "8b97e4a933a34f8fa33dcf914d438334",
            "198cb4b3be7246d4b4fa3ebf2d69cec6",
            "985e8878b3494f3eab7d21d29da25f67",
            "5919a07f814a42a88f633463080df9fa",
            "3360f31d17544775af244e0351e2e171",
            "29cc017eb0cd4350b2c69d576b8de3c5"
          ]
        },
        "id": "rVONk_0-XWba",
        "outputId": "b2ee7439-695a-4bf2-e68a-7130ced47f95"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3360f31d17544775af244e0351e2e171",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/17554 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29cc017eb0cd4350b2c69d576b8de3c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/5841 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def preprocess(example):\n",
        "    result = tokenizer.apply_chat_template(\n",
        "        [\n",
        "            {\"role\": \"user\", \"content\": example[\"question\"]},\n",
        "            {\"role\": \"model\", \"content\": example[\"answers\"][\"text\"][0]},\n",
        "        ],\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "    )\n",
        "    return result\n",
        "\n",
        "# 전처리 할때만 eos 토큰 추가\n",
        "added_template = \"{{ eos_token }}\"\n",
        "tokenizer.chat_template += added_template\n",
        "\n",
        "dataset = dataset.map(\n",
        "    preprocess,\n",
        "    num_proc=2,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        ")\n",
        "\n",
        "# 전처리 후 다시 제거\n",
        "tokenizer.chat_template = tokenizer.chat_template[:-len(added_template)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qcFasr4FK7bm",
      "metadata": {
        "id": "qcFasr4FK7bm"
      },
      "source": [
        "## QLoRA 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BmjJoCj25png",
      "metadata": {
        "id": "BmjJoCj25png"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/ckpt\",\n",
        "    max_steps=3000,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    warmup_ratio=0.03,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=100,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JWRPpomj5dyz",
      "metadata": {
        "id": "JWRPpomj5dyz"
      },
      "source": [
        "## 모델 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ruLgjeJXh_h8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruLgjeJXh_h8",
        "outputId": "f360826b-0a70-4411-fd5f-1a362be434f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained(\"/content/ko-gemma-2b-sum-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Wq8RN9mJ9I0z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq8RN9mJ9I0z",
        "outputId": "93a50248-38fe-4409-bf0f-e9864ecba99e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 38M\n",
            "-rw------- 1 root root  726 Jul 14 15:36 adapter_config.json\n",
            "-rw------- 1 root root  38M Jul 14 15:36 adapter_model.safetensors\n",
            "-rw------- 1 root root 5.0K Jul 14 15:36 README.md\n"
          ]
        }
      ],
      "source": [
        "!ls -alh /content/ko-gemma-2b-sum-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HEhyHPfbl6Ry",
      "metadata": {
        "id": "HEhyHPfbl6Ry"
      },
      "source": [
        "## 추론"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hEVF-x60MP3H",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230,
          "referenced_widgets": [
            "694191f2ccf841f38b7a35437c8e267b",
            "c99f608719954a098233cddba1b51162",
            "3c02058a1abe4927b245c68f73bed5ad",
            "1e44d11c9ae54722808c786b2cbb5806",
            "51ee16e710dd4ca38bca39cd961fdb8f",
            "586d821f07564bbe93ce45cee9011af8",
            "c188d3ff93ca47c49f3bd404aa7df70e",
            "02557acea50d496b9eeee053f780e92f",
            "6688383bea5c479287413cd603746724",
            "e031bd6577ee49dd8b1757450e889754",
            "61dd30cecf6e4873914fea9754725fb5"
          ]
        },
        "id": "hEVF-x60MP3H",
        "outputId": "4a379d25-f2c7-43c0-ffd1-efd296139087"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "694191f2ccf841f38b7a35437c8e267b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bos><start_of_turn>user\n",
            "다음 질문에 대해 대답해 주세요.\n",
            "질문 : 북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "13일<end_of_turn><eos>\n"
          ]
        }
      ],
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    GenerationConfig,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import PeftModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "# tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    \"/content/ko-gemma-2b-sum-v1\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"float16\",\n",
        ")\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# datasets\n",
        "dataset = load_dataset(\"klue\", \"mrc\")\n",
        "dataset[\"train\"][0]\n",
        "\n",
        "doc = dataset[\"train\"][\"question\"][0]\n",
        "messages = [{\"role\": \"user\", \"content\": doc}]\n",
        "\n",
        "# generate\n",
        "gen_cfg = GenerationConfig(\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.92,\n",
        "    return_full_text=False,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs.to(model.device),\n",
        "        generation_config=gen_cfg,\n",
        "    )\n",
        "    print(tokenizer.decode(outputs[0].cpu().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SH94eGRlPneO",
      "metadata": {
        "id": "SH94eGRlPneO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
   
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
