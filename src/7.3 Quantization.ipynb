{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di-PXebk7jdF"
      },
      "source": [
        "# install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCkp3G5PLhqC",
        "outputId": "5d047d8e-e22e-43d2-d19c-f6d91c16ac3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install \\\n",
        "     scikit-learn==1.4.2 \\\n",
        "     transformers==4.43.0 \\\n",
        "     datasets==2.20.0 \\\n",
        "     peft==0.10.0 \\\n",
        "     accelerate==0.32.1 \\\n",
        "     bitsandbytes==0.43.1 \\\n",
        "     trl==0.9.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCNyhdTG8HAe"
      },
      "source": [
        "# 7.3.1 bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t3KY_KFGkpL"
      },
      "source": [
        "### 8-bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435,
          "referenced_widgets": [
            "11727f5489f04783a9b76e31fe7edda9",
            "73271e5d50a44ece8f58a7a3059aac35",
            "3f99af50e6dc4c28a40065d46ea61872",
            "9c3c0299cee34a1c81b756b7edd50e29",
            "49d3447157404835824345afa122fc92",
            "ab31abf0bef549b8acab4e3e4513a072",
            "e868280ee5b54838947b20b76585d99d",
            "c6fb2109f87a4c27af4cc1eaa4a94038",
            "f9ce58de425548c3a942de8776b22c1a",
            "b83d9440aa79499ca6fac71d61949b64",
            "5a46b950767048cc93d9e578911c8433",
            "9a991d4334d3443f9ab442c809fc531c",
            "d56d7bd19ecc482186f6e1597fce1625",
            "6669f6219b08434d8a8f4ac39f341a2f",
            "a46bf280d6e94b00a208c9eecc58b843",
            "9a3b9063022d4b78ae3ea347e821819c",
            "59fcb93b120c446393fe130c43e71f9c",
            "3a2f04d69637471f8a42df1c3c3d8d67",
            "a39f95275f764baca133bf31fde3f7f8",
            "a8e76ea40613495eae17d5e6f758fb08",
            "ca023c3014ad4ff7b477777062198126",
            "2a35e31aeeb547fc966fb46bfa7691f8",
            "9fe19dcf77694ac1a9af3099067fc530",
            "0059c0936c2345199f09c700ee212bf8",
            "af431ce0f5784c8bb1a6ac436905ac22",
            "a12735c1318b4da9860d0a6e810f9afb",
            "9c78a4be5d894c8cbfe9c9292496fb3d",
            "2616c489d3db4e2ba74f136fa390c646",
            "8d37d365eea64606a30aa0bb93e61a7b",
            "7a9b5ffdba2d4afa9451d9e03c57e6df",
            "36a571c63a7f45e89eda0b640fe6377c",
            "d500f56bcd4c4f83811dbb49d45e750a",
            "186009e76932409ba476144325947657",
            "ac4c70be0d86493889d4f3ebf722c936",
            "8536ffe5201a4fcb906c99beb9c0b301",
            "e2fb01bff082480491ec765ccebcb210",
            "e1b20e2f29094e5399451cc7ea68fa20",
            "ba99d291e767437b97f60f150308c5d3",
            "bd5efce304a243d5814decc63af00bc3",
            "c5499846d7e848258313a6280f6279ed",
            "a4e030c66d4e4b7b925b9a6499d72df9",
            "86ca6f2254d24b969f68e40786308939",
            "7cab77c2cd6e449b97b00cb6de0ca3a9",
            "93a91be687e84068aa9dca42c251bd95",
            "72736b408e174a4391a504cfb387f510",
            "8160b6bfb9a84c198948dc97054cabe8",
            "60df9a41a8c94395a5cecbb11609d360",
            "3bd139ad078a4e169514659ab330e419",
            "a7ee1f5249144acfbc3a12856581db4e",
            "fcd1f8a7aa2342c88d7a0f7b31abeb5e",
            "edddd7c84fd54bb18d012c9502e0fbd3",
            "56548393a48b437888cb5e508f784ca8",
            "01a252a657af4e15afaacebb40eb7109",
            "ae534967c1e8487bae9c3e6ca06d2f2b",
            "3a10be87ee7740b7ad2c64a772eba365",
            "f582a42c190e4795811b78066467100d",
            "180df03609cd4a4ba87ada4626701c13",
            "effb631bdb4040749ddabf376c578fa0",
            "ce035f37030e4bd493d322a854ce798b",
            "e2411ced91974c859c009bacadcdf91e",
            "f798520a55f143fc8920b30328e02ec8",
            "40025de3bf2f4f9eb724c4081dcd6311",
            "08530e56234d4f6d9be6599f6b89a089",
            "01baab31ca314d49bc298792f4763a68",
            "7cf6dfa7357045af9f68bec6a6a0b6c2",
            "cfc92ceb90394399baeb2c6b6bdec676",
            "45d7294704ea4656bcdf8ce0f313bf83",
            "b9edc90e83864747be894997f57626a7",
            "3fbbf24228664dcb86b048095bf6c237",
            "6b6d8bc7a1184535b6733084e384780e",
            "1713701d14db4bd3956c4076d27362d9",
            "8d7743337a32499ba7c4335e3ba2f832",
            "797913c7cc7847908ef69c016695b1d0",
            "0ce733192b8a4246909a427f75cfcf1f",
            "72ed7f38de0f4483a3c51697335af14a",
            "ec8476941384469bae8b725bd2d51932",
            "a20cc68b17df45d6ba75e10a5b91f532"
          ]
        },
        "id": "P7b2fKT4685l",
        "outputId": "f425ef86-4eb0-4942-f8ca-2aec4daccb9f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11727f5489f04783a9b76e31fe7edda9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/676 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a991d4334d3443f9ab442c809fc531c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fe19dcf77694ac1a9af3099067fc530",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac4c70be0d86493889d4f3ebf722c936",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72736b408e174a4391a504cfb387f510",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f582a42c190e4795811b78066467100d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45d7294704ea4656bcdf8ce0f313bf83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 양자화 미사용\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nlpai-lab/ko-gemma-2b-v1\",\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "7354b1d9d50c4a8087628372b8e15cf6",
            "aa9610c48a7543f6ae68e8978080d25b",
            "38f5033a04324d898d04b312c13e046b",
            "2fe8b9be15cf414aaad6eb4ca76ae5e6",
            "17470243eea14af2b6c14921179ddcba",
            "a0100619a70240cda55d8315e183c882",
            "332e33bfa8c74e88a0a988041e52ae86",
            "3d0b966d8660488280e8ae9f99f84a41",
            "569467c87a9c4bb2849516e7726b7205",
            "dabe8065cb384150a616b392d351d812",
            "205662431a634d819d46debe79314ba8"
          ]
        },
        "id": "tjeVFdvsCaxs",
        "outputId": "435154e7-8283-463f-975e-8bc3cad40533"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7354b1d9d50c4a8087628372b8e15cf6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nlpai-lab/ko-gemma-2b-v1\",\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXjXLnboD-xy"
      },
      "source": [
        "오프로딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e455ee111ac8476198918976a81d5a3d",
            "c39e43cc94d4434fbdc066782e94e637",
            "4ce0656a25c04c27b7d229716b1c1035",
            "7d26b1fbd78d44ecbd103e6281e379ff",
            "36131ea384864b9caf6e2d3d66adf634",
            "fd0ae0887d9043cca2185c66ce68ef32",
            "09a6289356af4a239b3ded4515d2af01",
            "0fdcfc5ed2f14b9b8530d8958b48b6c0",
            "71133750e01d48b7a05e060892a43fb1",
            "f20401ad8c5f46f6bd7ad8d5e55b108d",
            "8207a6d7095643259399cd70b398c215"
          ]
        },
        "id": "B2CnketBOUmV",
        "outputId": "0c77967c-0d30-4add-d369-de37ce4420fd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e455ee111ac8476198918976a81d5a3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nlpai-lab/ko-gemma-2b-v1\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxOyQHBSKb6E"
      },
      "source": [
        "이상치 임계값"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c604a24bfb6c41ec91a46f5225e223bf",
            "f6b5cd65dfbe40a8a25a37048273a90e",
            "f809dd7d68f540f4b37c4e3fc9442abc",
            "d175bb4f843e468a98728a7d3236aca1",
            "ab5fc58cb8f14a93b2e009c14f54a70a",
            "e8e98c27cd2e4561b83aedbaae50e03f",
            "eae327d42ea043a9aa470b50ac923845",
            "8aefb7bdc51140f6bb5e234da0fc2f04",
            "a4954ccd1bac48ac86b47b1049321dd3",
            "7a9c3112461c4ffea23ab831cad655a7",
            "8b5944a8ecc045fe8f381010d61a7614"
          ]
        },
        "id": "VuiCbS42G5mI",
        "outputId": "9b68b112-fd1c-4449-dbf2-208033b17fc7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c604a24bfb6c41ec91a46f5225e223bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(llm_int8_threshold=10.0)\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nlpai-lab/ko-gemma-2b-v1\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRcSsi1HOU_T"
      },
      "source": [
        "모듈 변환 건너뛰기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f316c1d6fbfe43728f87298244a84dc1",
            "5d3a5344eb664f2b8b8f215028848189",
            "36fc41851bfe4b5386fddd2a63ac8718",
            "39ee2f9ddebe42468a64a114c0195347",
            "64adbb3952494fb0aecc295472b57cf6",
            "855b3dc44ffd43ccafe93c940e2a819c",
            "f716d32bfdb94a61b7808119ef70588c",
            "5bb5180458d840e4ab48cdbc5d6d0ddb",
            "465099c45b7646828303106e6ea5563e",
            "8a522397803e478bbbfafe73d9e280f7",
            "16cd748b207d4a569d9347c2cee666a1"
          ]
        },
        "id": "_TqJPIcNLnVV",
        "outputId": "d3600f13-f4b0-454f-97f4-32f7ba28efd2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f316c1d6fbfe43728f87298244a84dc1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(llm_int8_skip_modules=[\"lm_head\"])\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nlpai-lab/ko-gemma-2b-v1\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcDvH_z0QPrS"
      },
      "source": [
        "### 4-bit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrQ_VqSfRlf3"
      },
      "source": [
        "데이터 타입 계산하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "36572d6970dc4ab79e05f757b25285dc",
            "bb64f8f525c84e17938deae9af534268",
            "60d90f7dfd884e59b7d295ae0d0ac0bb",
            "b02ba6263d0e45f482c84c02efda44a2",
            "5414ac0169154bbdb80641047acce944",
            "fba9f5cf6c574ee4bc1d3bbdaaa5d674",
            "978a91ce92b54a53a24ae48a04c69407",
            "0dc184f7167248068d4be8cd5c0e0a75",
            "e812f27a02954c80bc01c0585fcb31fc",
            "2d0aad2096cf46ef9a8299f5edfc5633",
            "abdbc13aa5444e5b860c1d2e3a112a29"
          ]
        },
        "id": "tXhK9hYeOT5F",
        "outputId": "de82d073-e0f6-48e3-b714-a1eece3348e8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36572d6970dc4ab79e05f757b25285dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=\"bfloat16\",\n",
        ")\n",
        "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nlpai-lab/ko-gemma-2b-v1\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b8w_YAURnbJ"
      },
      "source": [
        "NF4(Normal Float 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "553cede856c24de7af44adb773bbe97c",
            "73ee80003bcf413eabb91060cecd38e7",
            "061b1766f7404e1295e8ec809be5f3b0",
            "9fe6dd7a570d43fdae7684bfbcd45cbb",
            "3d6cb77f446849a7ba6f3357277deb32",
            "044105388fd841aa9be6fec20bbbd2c2",
            "ce5e0f6242974d4f8610abeb20347272",
            "4edf377e5fc64e59b14741d27bff7287",
            "b23649d6cbec450fa9a3470fa2182fe0",
            "2ac736cf97ac4fc8afab0cb98afb8a43",
            "1aa4214807144922bc7340f204492f2d"
          ]
        },
        "id": "a7LKXj-9QVzb",
        "outputId": "40ea1b1f-3d87-42cd-f8f4-ad287a752638"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "553cede856c24de7af44adb773bbe97c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "model_nf4 = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nlpai-lab/ko-gemma-2b-v1\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=nf4_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZB6npY_SdoG"
      },
      "source": [
        "중첩 양자화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b17b8de583314a2ab0a7035553a0a1b0",
            "656ea405261b468e8e40921c029f85a2",
            "5e4817d4369744c3aa89859fe56ab1b8",
            "a2b498c4f1ff46baa0f7dbed7220fe43",
            "0c542f50e71e42c0b0cc0cd233a9a385",
            "262b9eef38694b9a96c58c98a210945f",
            "f56d4ec3cf104ac6a241f7b89ff6a968",
            "bf692d8d68a540d7b0416aa4f65ca107",
            "5bfab57193f148029cd1d397a306e8a5",
            "815865db13294ae6b6225262833c3e64",
            "948f313678cc4004af54fc30f45f5722"
          ]
        },
        "id": "thoI5rUNXJ4r",
        "outputId": "ce4d833a-918b-4bfc-bfeb-4e69133eabd4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b17b8de583314a2ab0a7035553a0a1b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "double_quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "model_double_quant = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nlpai-lab/ko-gemma-2b-v1\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=double_quant_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WXVT19eXlg4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
